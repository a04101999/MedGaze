{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38920a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import argparse\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "\n",
    "def fixations2seq(fixations,  max_len):\n",
    "    processed_fixs = []\n",
    "    for fix in fixations:\n",
    "        processed_fixs.append({'tgt_seq_y': torch.tensor(np.array(fix['Y'])[:max_len]), 'tgt_seq_x': torch.tensor(np.array(fix['X'])[:max_len]), 'tgt_seq_t': torch.tensor(np.array(fix['T'])[:max_len]),\n",
    "        'task': fix['task'], 'img_name':fix['name']}) \n",
    "    return processed_fixs\n",
    "\n",
    "    \n",
    "\n",
    "args={ \n",
    "    'head_lr':1e-6,\n",
    "    'tail_lr':1e-4, \n",
    "    'belly_lr':2e-6, \n",
    "    'dataset_dir': \"\",  # dataset path\n",
    "    'train_file':'train_egd_ref_128_dur.json',\n",
    "    'valid_file':'val_text_128_128_r_dur.json', \n",
    "    'img_ftrs_dir': \"image_features_text_qformer_llm_exp_128_full_eg_reflacx/\", #image features directory\n",
    "    'im_h':8, \n",
    "    'im_w':8,\n",
    "    'patch_size':16,\n",
    "    'seed':42, \n",
    "    'batch_size':32, \n",
    "    'epochs':500, \n",
    "    'max_len':50, \n",
    "    'num_encoder':6, \n",
    "    'num_decoder':6, \n",
    "    'hidden_dim':1408, \n",
    "    'nhead':8, \n",
    "    'img_hidden_dim':2048, \n",
    "    'lm_hidden_dim':768,\n",
    "    'encoder_dropout':0.1, \n",
    "    'decoder_dropout':0.2, \n",
    "    'cls_dropout':0.4, \n",
    "    'retraining':False, \n",
    "    \n",
    "    'model_root':'/home/cougarnet.uh.edu/aawasth3/Gazeformer/gazefromer_qformer_llm_using_rest_feaex_8x8.py_128_128/train_28-02-2024-14-34-40/gazeformer_6E_6D_32_1408d_300.pkg',  #model directory\n",
    "    'cuda':4, \n",
    "    'num_workers':6\n",
    "       \n",
    "   }\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbf79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class fixation_dataset(Dataset):\n",
    "    def __init__(self, fixs, img_ftrs_dir):\n",
    "        self.fixs = fixs\n",
    "        self.img_ftrs_dir = img_ftrs_dir\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fixs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        fixation = self.fixs[idx]\n",
    "\n",
    "        image_ftrs = torch.load(join(self.img_ftrs_dir, fixation['img_name'].replace('jpg', 'pth'))).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        return {'task': fixation['task'], 'tgt_y': fixation['tgt_seq_y'].float(), 'tgt_x': fixation['tgt_seq_x'].float(), 'tgt_t': fixation['tgt_seq_t'].float(),'src_img': image_ftrs,'imid':fixation['img_name'][:-4] }\n",
    "class COCOSearch18Collator(object):\n",
    "    def __init__(self, embedding_dict, max_len, im_h, im_w, patch_size):\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.max_len = max_len\n",
    "        self.im_h = im_h\n",
    "        self.im_w = im_w\n",
    "        self.patch_size = patch_size\n",
    "        self.PAD = [-3, -3, -3]\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_tgt_y = []\n",
    "        batch_tgt_x = []\n",
    "        batch_tgt_t = []\n",
    "        batch_imgs = []\n",
    "        batch_tasks = []\n",
    "        batch_ids=[]\n",
    "        \n",
    "        for t in batch:\n",
    "            #print(t.keys())\n",
    "            batch_tgt_y.append(t['tgt_y'])\n",
    "            batch_tgt_x.append(t['tgt_x'])\n",
    "            batch_tgt_t.append(t['tgt_t'])\n",
    "            batch_imgs.append(t['src_img'])\n",
    "            batch_tasks.append(self.embedding_dict[t['imid']])\n",
    "            batch_ids.append(t['imid'])\n",
    "        \n",
    "        batch_tgt_y.append(torch.zeros(self.max_len))\n",
    "        batch_tgt_x.append(torch.zeros(self.max_len))\n",
    "        batch_tgt_t.append(torch.zeros(self.max_len))\n",
    "        batch_tgt_y = pad_sequence(batch_tgt_y, padding_value=self.PAD[0])[:, :-1].unsqueeze(-1)\n",
    "        batch_tgt_x = pad_sequence(batch_tgt_x, padding_value=self.PAD[1])[:, :-1].unsqueeze(-1)\n",
    "        batch_tgt_t = pad_sequence(batch_tgt_t, padding_value=self.PAD[2])[:, :-1].unsqueeze(-1)\n",
    "        \n",
    "        batch_imgs = torch.cat(batch_imgs, dim = 0)\n",
    "        batch_tgt = torch.cat([batch_tgt_y, batch_tgt_x, batch_tgt_t], dim = -1).long().permute(1, 0, 2)\n",
    "        batch_firstfix = torch.tensor([(self.im_h//2)*self.patch_size, (self.im_w//2)*self.patch_size]).unsqueeze(0).repeat(batch_imgs.size(0), 1)\n",
    "        batch_tgt_padding_mask = batch_tgt[:, :, 0] == self.PAD[0]\n",
    "        \n",
    "        \n",
    "        return batch_imgs, batch_tgt, batch_tgt_padding_mask, batch_tasks, batch_firstfix,batch_ids\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ef3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_CONFIG_DICT = {\n",
    "        \"pretrain_opt2.7b\": \"configs/models/blip2/blip2_pretrain_opt2.7b.yaml\",\n",
    "        \"pretrain_opt6.7b\": \"configs/models/blip2/blip2_pretrain_opt6.7b.yaml\",\n",
    "        \"caption_coco_opt2.7b\": \"configs/models/blip2/blip2_caption_opt2.7b.yaml\",\n",
    "        \"caption_coco_opt6.7b\": \"configs/models/blip2/blip2_caption_opt6.7b.yaml\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c549953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "x=np.load(open( 'embeddings_text_egd_ref.npy', mode='rb'), allow_pickle = True)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uy=[]\n",
    "\n",
    "import json\n",
    "         \n",
    "path='full_egd_ref_128_dur.json'\n",
    "\n",
    "\n",
    "def js_r(filename: str):\n",
    "    with open(filename) as f_in:\n",
    "        return json.load(f_in)\n",
    "    \n",
    "fulld=js_r(path)\n",
    "new_dict = {item['name']:item for item in fulld}   \n",
    "#tasks=[]\n",
    "for i in x.item().items():\n",
    "           #print(i[0])\n",
    "    \n",
    "           x.item()[i[0]]=new_dict[i[0]+'.jpg']['task']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403058f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "\n",
    "class medgaze(nn.Module):\n",
    "    def __init__(self, transformer, spatial_dim, dropout=0.4, max_len = 7, patch_size  = 16, device = \"cuda:0\"):\n",
    "        super(medgaze, self).__init__()\n",
    "        self.spatial_dim = spatial_dim\n",
    "        self.transformer = transformer.to(device)\n",
    "        self.hidden_dim = transformer.d_model\n",
    "        #fixation embeddings\n",
    "        self.querypos_embed = nn.Embedding(max_len,self.hidden_dim).to(device)\n",
    "        #2D patch positional encoding\n",
    "        self.patchpos_embed = PositionEmbeddingSine2d(spatial_dim, hidden_dim=1408, normalize=True, device = device)\n",
    "        #2D pixel positional encoding for initial fixation\n",
    "        self.queryfix_embed = PositionEmbeddingSine2d((spatial_dim[0] * patch_size, spatial_dim[1] * patch_size), hidden_dim=self.hidden_dim, normalize=True, flatten = False, device = device).pos.to(device)\n",
    "        #classify fixation, or PAD tokens\n",
    "        self.token_predictor = nn.Linear(self.hidden_dim, 2).to(device)\n",
    "        #Gaussian parameters for x,y,t\n",
    "        self.generator_y_mu = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        self.generator_x_mu = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        self.generator_t_mu = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        self.generator_y_logvar = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        self.generator_x_logvar = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        self.generator_t_logvar = nn.Linear(self.hidden_dim, 1).to(device)\n",
    "        \n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.activation = F.relu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1).to(device)\n",
    "        #projection for first fixation encoding\n",
    "        self.firstfix_linear = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "    #reparameterization trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, task: Tensor):\n",
    "        src = src.to(self.device)\n",
    "        tgt_input = torch.zeros(self.max_len, src.size(0), self.hidden_dim).to(self.device)#Notice that this where we convert target input to zeros\n",
    "\n",
    "        tgt_input[0, :, :] = self.firstfix_linear(self.queryfix_embed[tgt[:, 0], tgt[:,1], :])\n",
    "        outs= self.transformer(src=src, tgt=tgt_input, tgt_mask= None, tgt_key_padding_mask = None, \n",
    "        task = task, querypos_embed = self.querypos_embed.weight.unsqueeze(1), patchpos_embed = self.patchpos_embed)\n",
    "\n",
    "        outs = self.dropout(outs)\n",
    "        #print('dhfbhsbdfjjbfjejfjenjfnewjnfjnejwnfjjwe')\n",
    "        #print(outs.shape)\n",
    "        #get Gaussian parameters for (x,y,t)\n",
    "        y_mu, y_logvar, x_mu, x_logvar, t_mu, t_logvar = self.generator_y_mu(outs),self.generator_y_logvar(outs), self.generator_x_mu(outs), self.generator_x_logvar(outs), self.generator_t_mu(outs), self.generator_t_logvar(outs)\n",
    "\n",
    "        return self.softmax(self.token_predictor(outs)), self.activation(self.reparameterize(y_mu, y_logvar)),self.activation(self.reparameterize(x_mu, x_logvar)), self.activation(self.reparameterize(t_mu, t_logvar))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bb7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionEmbeddingSine1d(nn.Module):\n",
    "    def __init__(self, max_len, hidden_dim=768, temperature=1000, normalize=False, scale=None, device = \"cuda:0\"):\n",
    "        super(PositionEmbeddingSine1d, self).__init__()\n",
    "        normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.device = device\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        if normalize:\n",
    "            eps = 1e-6\n",
    "            position = position / (max_len - 1 + eps) * scale\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2) * (-math.log(temperature) / hidden_dim))\n",
    "        self.pos = torch.zeros(max_len, hidden_dim)\n",
    "        self.pos[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pos[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos = self.pos.unsqueeze(1).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos[:x.size(0), :].to(self.device)\n",
    "        \n",
    "class PositionEmbeddingSine2d(nn.Module):\n",
    "    def __init__(self, spatial_dim, hidden_dim=768, temperature=10000, normalize=False, scale=None, flatten = True, device = \"cuda:0\"):\n",
    "        super(PositionEmbeddingSine2d, self).__init__()\n",
    "        self.num_pos_feats = hidden_dim // 2\n",
    "        normalize = normalize\n",
    "        self.h, self.w = spatial_dim\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.device = device\n",
    "        position_y = torch.arange(self.h).unsqueeze(1)\n",
    "        position_x = torch.arange(self.w).unsqueeze(1)\n",
    "        if normalize:\n",
    "            eps = 1e-6\n",
    "            position_y = position_y / (self.h - 1 + eps) * scale\n",
    "            position_x = position_x / (self.w - 1 + eps) * scale\n",
    "        div_term = torch.exp(torch.arange(0, self.num_pos_feats, 2).float() * (-math.log(temperature) / self.num_pos_feats))\n",
    "        pe_y = torch.zeros(self.h, 1, self.num_pos_feats)\n",
    "        pe_x = torch.zeros(1, self.w, self.num_pos_feats)\n",
    "        pe_y[:, 0, 0::2] = torch.sin(position_y * div_term)\n",
    "        pe_y[:, 0, 1::2] = torch.cos(position_y * div_term)\n",
    "        pe_x[0, :, 0::2] = torch.sin(position_x * div_term)\n",
    "        pe_x[0, :, 1::2] = torch.cos(position_x * div_term)\n",
    "        pe_y = pe_y.repeat(1, self.w, 1)\n",
    "        pe_x = pe_x.repeat(self.h, 1, 1)\n",
    "        self.pos = torch.cat((pe_y, pe_x), dim=-1).permute(2, 0, 1)\n",
    "        if flatten:\n",
    "            self.pos =  self.pos.view(hidden_dim, -1).permute(1,0).unsqueeze(1)\n",
    "        else:\n",
    "            self.pos = self.pos.permute(1,2,0)\n",
    "        del pe_y, pe_x, position_y, position_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.to(self.device) + self.pos.to(self.device)\n",
    "\n",
    "\n",
    "class FixationEmbeddingLearned2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, spatial_dim, hidden_dim = 768, device = \"cuda:0\"):\n",
    "        super(FixationEmbeddingLearned2d, self).__init__()\n",
    "        self.h, self.w = spatial_dim\n",
    "        self.row_embed = nn.Embedding(self.h, hidden_dim//2)\n",
    "        self.col_embed = nn.Embedding(self.w, hidden_dim//2)\n",
    "        self.device = device\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, token):\n",
    "        x_emb = self.col_embed(token[:, :, 1])\n",
    "        y_emb = self.row_embed(token[:, :, 0])\n",
    "        pos = torch.cat([y_emb, x_emb], dim = -1).to(self.device)\n",
    "        return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff66947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 16:29:23.764552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-04-23 16:29:23.764671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2024-04-23 16:29:23.764747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 16:29:23.764812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-23 16:29:23.764858: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "#ResNet-50 backbone\n",
    "class ResNetCOCO(nn.Module):\n",
    "    def __init__(self, device = \"cuda:0\"):\n",
    "        super(ResNetCOCO, self).__init__()\n",
    "        self.resnet = maskrcnn_resnet50_fpn(pretrained=True).backbone.body.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        x = x.to(self.device)\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        \n",
    "        bs, ch, _, _ = x.size()\n",
    "        x = x.view(bs, ch, -1).permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=512, img_hidden_dim = 2048, lm_dmodel = 768, nhead=8, num_encoder_layers=2,\n",
    "                 num_decoder_layers=6, dim_feedforward=512, encoder_dropout=0.1, decoder_dropout = 0.2, \n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False, device = \"cuda:0\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "      \n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                encoder_dropout, activation, normalize_before).to(device)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        #encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm).to(device)\n",
    "        input_proj = nn.Linear(img_hidden_dim, d_model).to(device)\n",
    "        encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm).to(device)\n",
    "\n",
    "        self.encoder =  TransformerEncoderWrapper(encoder, input_proj, encoder_dropout, device).to(device)\n",
    "      \n",
    "        #self.encoder =  TransformerEncoderWrapper(device).to(device)\n",
    "\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                decoder_dropout, activation, normalize_before).to(device)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,\n",
    "                                          return_intermediate=return_intermediate_dec).to(device)\n",
    "        \n",
    "        dropout = nn.Dropout(decoder_dropout)\n",
    "        self.decoder = TransformerDecoderWrapper(d_model, activation, decoder, dropout, device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.lm_dmodel = lm_dmodel\n",
    "        self.nhead = nhead\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, task:Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None, \n",
    "                querypos_embed: Optional[Tensor] = None, patchpos_embed: Optional[Tensor] = None):\n",
    "        \n",
    "        enoutput= self.encoder(src,  task)\n",
    "        \n",
    "        output = self.decoder(tgt, enoutput,  tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask,\n",
    "                              querypos_embed = querypos_embed, patchpos_embed = patchpos_embed)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "from lavis.common.dist_utils import download_cached_file, is_dist_avail_and_initialized       \n",
    "class TransformerEncoderWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, input_proj, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lip=Blip2OPT()\n",
    "        self.device = device\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.input_proj = input_proj.to(device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._reset_parameters(self.encoder)\n",
    "        self._reset_parameters(self.input_proj)\n",
    "      \n",
    "        \n",
    "    def _reset_parameters(self, mod):\n",
    "        for p in mod.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, task,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "                src_proj = self.input_proj(src).permute(1,0,2)#input projection from 2048 -> d\n",
    "\n",
    "                output = self.encoder(src_proj, mask=mask, src_key_padding_mask=src_key_padding_mask, patchpos_embed=patchpos_embed)#transformer encoder\n",
    "                #url_or_filename='https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth'\n",
    "                #cached_file = download_cached_file(\n",
    "                #url_or_filename, check_hash=True, progress=True\n",
    "            #)\n",
    "                #checkpoint = torch.load(cached_file, map_location=torch.device('cuda:3'))\n",
    "\n",
    "\n",
    "                #state_dict=checkpoint['model']\n",
    "                #self.lip.load_state_dict(state_dict, strict=False)\n",
    "                outputs=self.lip(output,task)\n",
    "        \n",
    "                return outputs\n",
    "\n",
    "\n",
    "class TransformerDecoderWrapper(nn.Module):\n",
    "    def __init__(self, d_model, activation, decoder, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.dropout = dropout\n",
    "        self.projd=nn.Linear(64,640)\n",
    "        self.linear = nn.Linear(2560, d_model)\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else pos(tensor)\n",
    "\n",
    "    def forward(self,tgt,  memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                querypos_embed: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        #vision-semantic joint embedding\n",
    "        #print(memory.shape)\n",
    "        memory_task = self.dropout(self.activation(self.linear(memory)))\n",
    "        #mem=self.projd(memory.permute(0,2,1))\n",
    "        memory_task=memory_task.permute(1,0,2)\n",
    "        \n",
    "        #decoder\n",
    "        #print('memory_task')\n",
    "        #print(memory_task.shape)\n",
    "        #print('dbfjdf')\n",
    "        output = self.decoder(tgt, memory_task, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask,\n",
    "                              querypos_embed = querypos_embed, patchpos_embed = patchpos_embed)\n",
    "        return output\n",
    "        \n",
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                querypos_embed: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           querypos_embed = querypos_embed, \n",
    "                           patchpos_embed = patchpos_embed)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(self.norm(output))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.pop()\n",
    "                intermediate.append(output)\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            return torch.stack(intermediate)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else pos + tensor\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                querypos_embed: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        \n",
    "        q = k = v = self.with_pos_embed(tgt, querypos_embed)\n",
    "        tgt2 = self.self_attn(q, k, value=v, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, querypos_embed),\n",
    "                                   key=patchpos_embed(memory),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                querypos_embed: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = v = self.with_pos_embed(tgt2, querypos_embed)\n",
    "        tgt2 = self.self_attn(q, k, value=v, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, querypos_embed),\n",
    "                                   key=patchpos_embed(memory),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                querypos_embed: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, \n",
    "                           querypos_embed = querypos_embed, \n",
    "                           patchpos_embed = patchpos_embed)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, \n",
    "                           querypos_embed = querypos_embed, \n",
    "                           patchpos_embed = patchpos_embed)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                patchpos_embed: Optional[Tensor] = None):\n",
    "        output = src\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, pos=patchpos_embed, src_mask = mask, src_key_padding_mask = src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else pos(tensor)\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     patchpos_embed: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, patchpos_embed)\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
    "\n",
    "        \n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef843f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Copyright (c) 2023, salesforce.com, inc.\n",
    " All rights reserved.\n",
    " SPDX-License-Identifier: BSD-3-Clause\n",
    " For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n",
    "\"\"\"\n",
    "import logging\n",
    "from packaging import version\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "import torch.nn as nn\n",
    "\n",
    "from lavis.common.registry import registry\n",
    "from lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\n",
    "# from lavis.models.blip2_models.modeling_opt import OPTForCausalLM, OPTConfig\n",
    "from transformers import AutoTokenizer, OPTConfig\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "class Blip2OPT(Blip2Base):\n",
    "    \"\"\"\n",
    "    BLIP2 OPT model.\n",
    "    Supported model types:\n",
    "        - pretrained_opt2.7b: pretrained model with OPT2.7b\n",
    "        - pretrained_opt6.7b: pretrained model with OPT6.7b\n",
    "        - caption_coco_opt2.7b: fintuned image captioning model with OPT2.7b\n",
    "        - caption_coco_opt6.7b: fintuned image captioning model with OPT6.7b\n",
    "    Usage:\n",
    "        >>> from lavis.models import load_model\n",
    "        >>> model = load_model(\"blip2_opt\", \"caption_coco_opt2.7b\")\n",
    "    \"\"\"\n",
    "\n",
    "    PRETRAINED_MODEL_CONFIG_DICT = {\n",
    "        \"pretrain_opt2.7b\": \"configs/models/blip2/blip2_pretrain_opt2.7b.yaml\",\n",
    "        \"pretrain_opt6.7b\": \"configs/models/blip2/blip2_pretrain_opt6.7b.yaml\",\n",
    "        \"caption_coco_opt2.7b\": \"configs/models/blip2/blip2_caption_opt2.7b.yaml\",\n",
    "        \"caption_coco_opt6.7b\": \"configs/models/blip2/blip2_caption_opt6.7b.yaml\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vit_model=\"eva_clip_g\",\n",
    "        img_size=364,\n",
    "        drop_path_rate=0,\n",
    "        use_grad_checkpoint=False,\n",
    "        vit_precision=\"fp16\",\n",
    "        freeze_vit=True,\n",
    "        num_query_token=32,\n",
    "        opt_model=\"facebook/opt-2.7b\",\n",
    "        prompt=\"\",\n",
    "        max_txt_len=32,\n",
    "        apply_lemmatizer=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        apply_lemmatizer: when set to True, postprocess predict_answers() result with lemmas.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        transformers_version = version.parse(transformers.__version__)\n",
    "        assert transformers_version >= version.parse(\"4.27\"), \"BLIP-2 OPT requires transformers>=4.27\"\n",
    "        \n",
    "        self.tokenizer = self.init_tokenizer()\n",
    "\n",
    "        \n",
    "\n",
    "        self.Qformer, self.query_tokens = self.init_Qformer(\n",
    "            num_query_token, 1408\n",
    "        )\n",
    "        self.Qformer.cls = None\n",
    "        self.Qformer.bert.embeddings.word_embeddings = None\n",
    "        self.Qformer.bert.embeddings.position_embeddings = None\n",
    "        for layer in self.Qformer.bert.encoder.layer:\n",
    "            layer.output = None\n",
    "            layer.intermediate = None\n",
    "\n",
    "        self.opt_tokenizer = AutoTokenizer.from_pretrained(opt_model, use_fast=False)\n",
    "        self.opt_model = OPTForCausalLM.from_pretrained(\n",
    "            opt_model, torch_dtype=torch.float16\n",
    "        )\n",
    "        for name, param in self.opt_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.eos_token_id = self.opt_tokenizer(\n",
    "            \"\\n\", add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "\n",
    "        self.opt_proj = nn.Linear(\n",
    "            self.Qformer.config.hidden_size, self.opt_model.config.hidden_size\n",
    "        )\n",
    "\n",
    "        self.max_txt_len = max_txt_len\n",
    "        self.prompt = prompt\n",
    "        prompt_tokens = self.opt_tokenizer(self.prompt, return_tensors=\"pt\")\n",
    "        self.prompt_length = prompt_tokens.attention_mask.sum(1)\n",
    "        \n",
    "        self._apply_lemmatizer = apply_lemmatizer\n",
    "        self._lemmatizer = None       \n",
    "\n",
    "    def forward(self, img,task):\n",
    "        image =img.permute(1,0,2)\n",
    "        text = task[0:len(image)]\n",
    "        \n",
    "\n",
    "        image_embeds = image\n",
    "        \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n",
    "            image.device\n",
    "        )\n",
    "\n",
    "        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "        #print(\"image dimension _embedidng qformer \")\n",
    "        #print(image_embeds.shape)\n",
    "        \n",
    "        query_output = self.Qformer.bert(\n",
    "            query_embeds=query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_atts,\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        inputs_opt = self.opt_proj(query_output.last_hidden_state)\n",
    "        atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(image.device)\n",
    "\n",
    "        self.opt_tokenizer.padding_side = \"right\"\n",
    "\n",
    "        text = [t + \"\\n\" for t in task]\n",
    "        #print(text)\n",
    "        opt_tokens = self.opt_tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_txt_len,\n",
    "        ).to(image.device)\n",
    "\n",
    "        #print(\"some_sus\")\n",
    "        #print(inputs_opt.shape)\n",
    "       \n",
    "        \n",
    "        inputs_embeds = self.opt_model.model.decoder.embed_tokens(opt_tokens.input_ids)\n",
    "        #print(inputs_embeds.shape)\n",
    "        inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n",
    "        #print(inputs_embeds.shape)\n",
    "        attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n",
    "\n",
    "        with self.maybe_autocast():\n",
    "            outputs = self.opt_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "                \n",
    "            )\n",
    "        \n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        samples,\n",
    "        use_nucleus_sampling=False,\n",
    "        num_beams=5,\n",
    "        max_length=30,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "            use_nucleus_sampling (bool): Whether to use nucleus sampling. If False, use top-k sampling.\n",
    "            num_beams (int): Number of beams for beam search. 1 means no beam search.\n",
    "            max_length (int): The maximum length of the sequence to be generated.\n",
    "            min_length (int): The minimum length of the sequence to be generated.\n",
    "            top_p (float): The cumulative probability for nucleus sampling.\n",
    "            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n",
    "            num_captions (int): Number of captions to be generated for each image.\n",
    "        Returns:\n",
    "            captions (list): A list of strings of length batch_size * num_captions.\n",
    "        \"\"\"\n",
    "        image = samples[\"image\"]\n",
    "        with self.maybe_autocast():\n",
    "            image_embeds = self.ln_vision(self.visual_encoder(image))\n",
    "            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n",
    "                image.device\n",
    "            )\n",
    "\n",
    "            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=image_embeds,\n",
    "                encoder_attention_mask=image_atts,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            inputs_opt = self.opt_proj(query_output.last_hidden_state)\n",
    "            atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(\n",
    "                image.device\n",
    "            )\n",
    "\n",
    "            if \"prompt\" in samples.keys():\n",
    "                prompt = samples[\"prompt\"]\n",
    "            else:\n",
    "                prompt = self.prompt\n",
    "\n",
    "            prompt = [prompt] * image.size(0)\n",
    "\n",
    "            opt_tokens = self.opt_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_len\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_txt_len,\n",
    "            ).to(image.device)\n",
    "            attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n",
    "            \n",
    "            # new version for transformers>=4.27\n",
    "            inputs_embeds = self.opt_model.get_input_embeddings()(opt_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat([inputs_opt,inputs_embeds],dim=1)\n",
    "            \n",
    "            outputs = self.opt_model.generate(\n",
    "                inputs_embeds=inputs_embeds, \n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=use_nucleus_sampling,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "            )\n",
    "            output_text = self.opt_tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )\n",
    "                            \n",
    "            # previous version for transformers<4.27\n",
    "            # if use_nucleus_sampling:\n",
    "            #     query_embeds = inputs_opt.repeat_interleave(num_captions, dim=0)\n",
    "            #     num_beams = 1\n",
    "            # else:\n",
    "            #     query_embeds = inputs_opt.repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "            # outputs = self.opt_model.generate(\n",
    "            #     input_ids=input_ids,\n",
    "            #     query_embeds=query_embeds,\n",
    "            #     attention_mask=attention_mask,\n",
    "            #     do_sample=use_nucleus_sampling,\n",
    "            #     top_p=top_p,\n",
    "            #     temperature=temperature,\n",
    "            #     num_beams=num_beams,\n",
    "            #     max_new_tokens=max_length,\n",
    "            #     min_length=min_length,\n",
    "            #     eos_token_id=self.eos_token_id,\n",
    "            #     repetition_penalty=repetition_penalty,\n",
    "            #     length_penalty=length_penalty,\n",
    "            #     num_return_sequences=num_captions,\n",
    "            # )\n",
    "\n",
    "            # prompt_length = opt_tokens.input_ids.shape[1]\n",
    "            # output_text = self.opt_tokenizer.batch_decode(\n",
    "            #     outputs[:, prompt_length:], skip_special_tokens=True\n",
    "            # )\n",
    "            \n",
    "            output_text = [text.strip() for text in output_text]\n",
    "            return output_text\n",
    "        \n",
    "        \n",
    "    def predict_answers(\n",
    "        self,\n",
    "        samples,\n",
    "        num_beams=5,\n",
    "        inference_method=\"generate\",\n",
    "        max_len=10,\n",
    "        min_len=1,\n",
    "        num_ans_candidates=128,\n",
    "        answer_list=None,\n",
    "        prompt=\"\",\n",
    "        length_penalty=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        image = samples[\"image\"]\n",
    "        with self.maybe_autocast():\n",
    "            image_embeds = self.ln_vision(self.visual_encoder(image))\n",
    "            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n",
    "                image.device\n",
    "            )\n",
    "\n",
    "            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "            query_output = self.Qformer.bert(\n",
    "                query_embeds=query_tokens,\n",
    "                encoder_hidden_states=image_embeds,\n",
    "                encoder_attention_mask=image_atts,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            inputs_opt = self.opt_proj(query_output.last_hidden_state)\n",
    "            atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(\n",
    "                image.device\n",
    "            )\n",
    "\n",
    "            if isinstance(samples[\"text_input\"], str):\n",
    "                samples[\"text_input\"] = [samples[\"text_input\"]]\n",
    "            if prompt:\n",
    "                text_input = [prompt.format(question) for question in samples[\"text_input\"]]\n",
    "            else:\n",
    "                text_input = samples[\"text_input\"]\n",
    "\n",
    "            self.opt_tokenizer.padding_side = \"left\"\n",
    "            opt_tokens = self.opt_tokenizer(\n",
    "                text_input,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_txt_len,\n",
    "            ).to(image.device)\n",
    "        \n",
    "            attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n",
    "            \n",
    "            # require transformers>=4.27\n",
    "            inputs_embeds = self.opt_model.get_input_embeddings()(opt_tokens.input_ids)\n",
    "            inputs_embeds = torch.cat([inputs_opt,inputs_embeds],dim=1)\n",
    "            \n",
    "            outputs = self.opt_model.generate(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=False,\n",
    "                num_beams=num_beams,\n",
    "                max_new_tokens=max_len,\n",
    "                min_length=min_len,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                length_penalty=length_penalty,\n",
    "            )\n",
    "            output_text = self.opt_tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )\n",
    "            output_text = [text.strip() for text in output_text]\n",
    "        if self._apply_lemmatizer or (\"apply_lemmatizer\" in samples.keys() and samples[\"apply_lemmatizer\"]):\n",
    "            output_text = self._lemmatize(output_text)\n",
    "\n",
    "        return output_text\n",
    "    \n",
    "    def _lemmatize(self, answers):\n",
    "        def apply(answer):\n",
    "            doc = self.lemmatizer(answer)\n",
    "\n",
    "            words = []\n",
    "            for token in doc:\n",
    "                if token.pos_ in [\"NOUN\", \"VERB\"]:\n",
    "                    words.append(token.lemma_)\n",
    "                else:\n",
    "                    words.append(token.text)\n",
    "            answer = \" \".join(words)\n",
    "\n",
    "            return answer\n",
    "\n",
    "        return [apply(answer) for answer in answers]\n",
    "\n",
    "    @property\n",
    "    def lemmatizer(self):\n",
    "        if self._lemmatizer is None:\n",
    "            try:\n",
    "                import spacy\n",
    "\n",
    "                self._lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "            except ImportError:\n",
    "                logging.error(\n",
    "                    \"\"\"\n",
    "                    Please install spacy and en_core_web_sm model to apply lemmatization.\n",
    "                    python -m spacy download en_core_web_sm\n",
    "                    OR\n",
    "                    import spacy.cli\n",
    "                    spacy.cli.download(\"en_core_web_sm\")\n",
    "                    \"\"\"\n",
    "                )\n",
    "                exit(1)\n",
    "\n",
    "        return self._lemmatizer\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        vit_model = cfg.get(\"vit_model\", \"eva_clip_g\")\n",
    "        img_size = cfg.get(\"image_size\")\n",
    "        num_query_token = cfg.get(\"num_query_token\")\n",
    "        opt_model = cfg.get(\"opt_model\")\n",
    "\n",
    "        drop_path_rate = cfg.get(\"drop_path_rate\", 0)\n",
    "        use_grad_checkpoint = cfg.get(\"use_grad_checkpoint\", False)\n",
    "        vit_precision = cfg.get(\"vit_precision\", \"fp16\")\n",
    "        freeze_vit = cfg.get(\"freeze_vit\", True)\n",
    "\n",
    "        prompt = cfg.get(\"prompt\", \"\")\n",
    "        max_txt_len = cfg.get(\"max_txt_len\", 32)\n",
    "        \n",
    "        apply_lemmatizer = cfg.get(\"apply_lemmatizer\", False)\n",
    "\n",
    "        model = cls(\n",
    "            vit_model=vit_model,\n",
    "            img_size=img_size,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            use_grad_checkpoint=use_grad_checkpoint,\n",
    "            vit_precision=vit_precision,\n",
    "            freeze_vit=freeze_vit,\n",
    "            num_query_token=num_query_token,\n",
    "            opt_model=opt_model,\n",
    "            prompt=prompt,\n",
    "            max_txt_len=max_txt_len,\n",
    "            apply_lemmatizer=apply_lemmatizer,\n",
    "        )\n",
    "        model.load_checkpoint_from_config(cfg)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f395307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" PyTorch OPT model.\"\"\"\n",
    "import random\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.models.opt.configuration_opt import OPTConfig\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"facebook/opt-350m\"\n",
    "_CONFIG_FOR_DOC = \"OPTConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"GPT2Tokenizer\"\n",
    "\n",
    "# Base model docstring\n",
    "_EXPECTED_OUTPUT_SHAPE = [1, 8, 1024]\n",
    "\n",
    "# SequenceClassification docstring\n",
    "_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"ArthurZ/opt-350m-dummy-sc\"\n",
    "_SEQ_CLASS_EXPECTED_LOSS = 1.71\n",
    "_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_0'\"\n",
    "\n",
    "# QuestionAnswering docstring\n",
    "_QA_EXPECTED_OUTPUT = \"'a nice puppet'\"\n",
    "_QA_EXPECTED_LOSS = 7.41\n",
    "_QA_TARGET_START_INDEX = 14\n",
    "_QA_TARGET_END_INDEX = 15\n",
    "\n",
    "OPT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"facebook/opt-125m\",\n",
    "    \"facebook/opt-350m\",\n",
    "    \"facebook/opt-1.3b\",\n",
    "    \"facebook/opt-2.7b\",\n",
    "    \"facebook/opt-6.7b\",\n",
    "    \"facebook/opt-13b\",\n",
    "    \"facebook/opt-30b\",\n",
    "    # See all OPT models at https://huggingface.co/models?filter=opt\n",
    "]\n",
    "\n",
    "\n",
    "def _make_causal_mask(\n",
    "    input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Make causal mask used for bi-directional self-attention.\n",
    "    \"\"\"\n",
    "    bsz, tgt_len = input_ids_shape\n",
    "    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
    "    mask_cond = torch.arange(mask.size(-1))\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "    mask = mask.to(dtype)\n",
    "\n",
    "    if past_key_values_length > 0:\n",
    "        mask = torch.cat(\n",
    "            [torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1\n",
    "        )\n",
    "    return mask[None, None, :, :].expand(\n",
    "        bsz, 1, tgt_len, tgt_len + past_key_values_length\n",
    "    )\n",
    "\n",
    "\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "\n",
    "    return inverted_mask.masked_fill(\n",
    "        inverted_mask.to(torch.bool), torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "\n",
    "class OPTLearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
    "        # and adjust num_embeddings appropriately. Other models don't have this hack\n",
    "        self.offset = 2\n",
    "        super().__init__(num_embeddings + self.offset, embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, attention_mask: torch.LongTensor, past_key_values_length: int = 0\n",
    "    ):\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        attention_mask = attention_mask.long()\n",
    "\n",
    "        # create positions depending on attention_mask\n",
    "        positions = (\n",
    "            torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask\n",
    "        ).long() - 1\n",
    "\n",
    "        # cut positions if `past_key_values_length` is > 0\n",
    "        positions = positions[:, past_key_values_length:]\n",
    "\n",
    "        return super().forward(positions + self.offset)\n",
    "\n",
    "\n",
    "class OPTAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        if (self.head_dim * num_heads) != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
    "                f\" and `num_heads`: {num_heads}).\"\n",
    "            )\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return (\n",
    "            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = (\n",
    "                attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "                + attention_mask\n",
    "            )\n",
    "            attn_weights = torch.max(\n",
    "                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n",
    "        if attn_weights.dtype == torch.float16:\n",
    "            attn_weights = nn.functional.softmax(\n",
    "                attn_weights, dim=-1, dtype=torch.float32\n",
    "            ).to(torch.float16)\n",
    "        else:\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(\n",
    "                bsz, self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(\n",
    "                bsz, self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "            attn_weights = attn_weights_reshaped.view(\n",
    "                bsz * self.num_heads, tgt_len, src_len\n",
    "            )\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        attn_probs = nn.functional.dropout(\n",
    "            attn_weights, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned aross GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "\n",
    "\n",
    "class OPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = OPTAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.do_layer_norm_before = config.do_layer_norm_before\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim)\n",
    "        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "    ) -> Tuple[\n",
    "        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`, *optional*): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
    "        \"\"\"\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
    "        if self.do_layer_norm_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # 350m applies layer norm AFTER attention\n",
    "        if not self.do_layer_norm_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Fully Connected\n",
    "        hidden_states_shape = hidden_states.shape\n",
    "        hidden_states = hidden_states.reshape(-1, hidden_states.size(-1))\n",
    "        residual = hidden_states\n",
    "\n",
    "        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n",
    "        if self.do_layer_norm_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(\n",
    "            hidden_states, p=self.dropout, training=self.training\n",
    "        )\n",
    "\n",
    "        hidden_states = (residual + hidden_states).view(hidden_states_shape)\n",
    "\n",
    "        # 350m applies layer norm AFTER attention\n",
    "        if not self.do_layer_norm_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "OPT_START_DOCSTRING = r\"\"\"\n",
    "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
    "    etc.)\n",
    "\n",
    "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
    "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
    "    and behavior.\n",
    "\n",
    "    Parameters:\n",
    "        config ([`OPTConfig`]):\n",
    "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
    "            load the weights associated with the model, only the configuration. Check out the\n",
    "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
    "    OPT_START_DOCSTRING,\n",
    ")\n",
    "class OPTPreTrainedModel(PreTrainedModel):\n",
    "\n",
    "    config_class = OPTConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"OPTDecoderLayer\"]\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, (OPTDecoder)):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "OPT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
    "            it.\n",
    "\n",
    "            Indices can be obtained using [`GPT2Tokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "\n",
    "            Indices can be obtained using [`OPTTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n",
    "            `past_key_values`).\n",
    "\n",
    "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
    "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
    "            information on the default strategy.\n",
    "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
    "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
    "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
    "\n",
    "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
    "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class OPTDecoder(OPTPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: OPTConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__(config)\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.layerdrop\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.max_target_positions = config.max_position_embeddings\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            config.vocab_size, config.word_embed_proj_dim, self.padding_idx\n",
    "        )\n",
    "        self.embed_positions = OPTLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings, config.hidden_size\n",
    "        )\n",
    "\n",
    "        if config.word_embed_proj_dim != config.hidden_size:\n",
    "            self.project_out = nn.Linear(\n",
    "                config.hidden_size, config.word_embed_proj_dim, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.project_out = None\n",
    "\n",
    "        if config.word_embed_proj_dim != config.hidden_size:\n",
    "            self.project_in = nn.Linear(\n",
    "                config.word_embed_proj_dim, config.hidden_size, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.project_in = None\n",
    "\n",
    "        # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n",
    "        # with checkpoints that have been fine-tuned before transformers v4.20.1\n",
    "        # see https://github.com/facebookresearch/metaseq/pull/164\n",
    "        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n",
    "            self.final_layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        else:\n",
    "            self.final_layer_norm = None\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
    "    def _prepare_decoder_attention_mask(\n",
    "        self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "    ):\n",
    "        # create causal mask\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(\n",
    "                input_shape,\n",
    "                inputs_embeds.dtype,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            ).to(inputs_embeds.device)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            expanded_attn_mask = _expand_mask(\n",
    "                attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "            ).to(inputs_embeds.device)\n",
    "            combined_attention_mask = (\n",
    "                expanded_attn_mask\n",
    "                if combined_attention_mask is None\n",
    "                else expanded_attn_mask + combined_attention_mask\n",
    "            )\n",
    "\n",
    "        return combined_attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        query_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`OPTTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        print(\"printing input embeddingskfksf\")\n",
    "        #print(inputs_embeds.shape)\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\n",
    "            )\n",
    "\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if query_embeds is not None:\n",
    "            inputs_embeds = torch.cat([query_embeds, inputs_embeds], dim=1)\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        # embed positions\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                inputs_embeds.shape[:2], dtype=torch.bool, device=inputs_embeds.device\n",
    "            )\n",
    "        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)\n",
    "\n",
    "        attention_mask = self._prepare_decoder_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        if self.project_in is not None:\n",
    "            inputs_embeds = self.project_in(inputs_embeds)\n",
    "        #print(\"sabfnabfnabsnfbnasbnfbansnfansbf\")\n",
    "        #print(inputs_embeds.shape)\n",
    "        hidden_states = inputs_embeds + pos_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                if attn_mask.size()[0] != (len(self.layers)):\n",
    "                    raise ValueError(\n",
    "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                        f\" {head_mask.size()[0]}.\"\n",
    "                    )\n",
    "\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):\n",
    "                continue\n",
    "\n",
    "            past_key_value = (\n",
    "                past_key_values[idx] if past_key_values is not None else None\n",
    "            )\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        # None for past_key_value\n",
    "                        return module(*inputs, output_attentions, None)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(decoder_layer),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    None,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        if self.final_layer_norm is not None:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        if self.project_out is not None:\n",
    "            hidden_states = self.project_out(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare OPT Model outputting raw hidden-states without any specific head on top.\",\n",
    "    OPT_START_DOCSTRING,\n",
    ")\n",
    "class OPTModel(OPTPreTrainedModel):\n",
    "    def __init__(self, config: OPTConfig):\n",
    "        super().__init__(config)\n",
    "        self.decoder = OPTDecoder(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.decoder.embed_tokens = value\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(OPT_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPast,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        expected_output=_EXPECTED_OUTPUT_SHAPE,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        query_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            query_embeds=query_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs\n",
    "\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            hidden_states=decoder_outputs.hidden_states,\n",
    "            attentions=decoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class OPTForCausalLM(OPTPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = OPTModel(config)\n",
    "\n",
    "        # the lm_head weight is automatically tied to the embed tokens weight\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.word_embed_proj_dim, config.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.decoder.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model.decoder = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.decoder\n",
    "\n",
    "    @replace_return_docstrings(\n",
    "        output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        query_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        reduction: Optional[str] = \"mean\",\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using [`OPTTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(num_hidden_layers, num_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "\n",
    "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
    "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
    "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
    "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
    "                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
    "\n",
    "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
    "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
    "\n",
    "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
    "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
    "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "            use_cache (`bool`, *optional*):\n",
    "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
    "                (see `past_key_values`).\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "\n",
    "        >>> model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "        >>> tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            query_embeds=query_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        #print('i am good boy')\n",
    "        #print(outputs[0].shape)\n",
    "        #print(outputs[0])\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=outputs[0],\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        query_embeds=None,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        use_cache=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            if input_ids is not None:\n",
    "                attention_mask = input_ids.new_ones(input_ids.shape)\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "            query_embeds = None\n",
    "        # first step, decoder_cached_states are empty\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"query_embeds\": query_embeds,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": use_cache,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (\n",
    "                tuple(\n",
    "                    past_state.index_select(0, beam_idx) for past_state in layer_past\n",
    "                ),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1769fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f81dacd1df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from timeit import default_timer as timer\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51246ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 scanpaths per test case...\n",
      "52f7cf0a-e13e2328-993ae39b-37ed0efb-14d64e8a.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "20\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.39003126e-02]\n",
      " [4.50500679e+01 6.16834145e+01 1.63514152e-01]\n",
      " [4.88856277e+01 6.89501801e+01 1.06468417e-01]\n",
      " [5.37727928e+01 7.42165527e+01 1.51218161e-01]\n",
      " [4.19684067e+01 6.87874908e+01 2.64457703e-01]\n",
      " [3.77303047e+01 7.27839508e+01 2.47878715e-01]\n",
      " [4.94224854e+01 7.43292160e+01 8.80792513e-02]\n",
      " [5.04514389e+01 6.97455902e+01 1.38577521e-01]\n",
      " [4.89332199e+01 6.03635330e+01 1.81452006e-01]\n",
      " [4.73007698e+01 8.13211823e+01 8.55107754e-02]\n",
      " [4.38756027e+01 7.61584167e+01 1.97015688e-01]\n",
      " [5.55883713e+01 7.22223816e+01 1.03697851e-01]\n",
      " [6.54869766e+01 8.40243683e+01 7.20555335e-02]\n",
      " [5.06415367e+01 8.13853683e+01 7.27019832e-02]\n",
      " [5.39352875e+01 9.69037018e+01 2.17110068e-02]\n",
      " [5.55966492e+01 8.44012604e+01 0.00000000e+00]\n",
      " [4.57314224e+01 8.60734177e+01 0.00000000e+00]\n",
      " [7.16135406e+01 3.67091370e+01 2.16672756e-02]\n",
      " [5.10819626e+01 5.68978615e+01 2.01099943e-02]\n",
      " [5.12290916e+01 1.07932732e+02 0.00000000e+00]]\n",
      "447aa937-aaa5b92b-23588e36-b056294e-ccd8091d.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "21\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 7.86045343e-02]\n",
      " [5.52444191e+01 6.86878128e+01 9.74756628e-02]\n",
      " [4.31208534e+01 3.69752884e+01 2.70290375e-01]\n",
      " [3.24265060e+01 5.34015884e+01 2.01747969e-01]\n",
      " [2.77256927e+01 6.85100479e+01 1.56901106e-01]\n",
      " [5.95245247e+01 7.16131439e+01 5.14776893e-02]\n",
      " [4.89395180e+01 8.11258011e+01 1.79275274e-01]\n",
      " [4.46453972e+01 8.12493515e+01 3.24753076e-01]\n",
      " [5.21883621e+01 6.07926712e+01 1.50275737e-01]\n",
      " [6.52996063e+01 6.68504410e+01 2.26989090e-02]\n",
      " [7.62206650e+01 6.55149078e+01 0.00000000e+00]\n",
      " [3.37367020e+01 6.68659897e+01 1.76793098e-01]\n",
      " [6.21142159e+01 6.81156540e+01 1.00122817e-01]\n",
      " [5.94212570e+01 7.90454559e+01 2.58982599e-01]\n",
      " [4.15948181e+01 6.54186096e+01 2.61776477e-01]\n",
      " [4.20316162e+01 7.19709854e+01 1.93623349e-01]\n",
      " [5.37952995e+01 7.86020203e+01 1.39665768e-01]\n",
      " [2.90824680e+01 7.36147385e+01 3.10227066e-01]\n",
      " [4.23391380e+01 7.87501068e+01 0.00000000e+00]\n",
      " [4.64493637e+01 5.59319572e+01 1.91568807e-01]\n",
      " [3.03626633e+01 4.07010117e+01 1.50393426e-01]]\n",
      "5dcc63b5-8adb28b9-2d439843-cfa4d608-d49b24f5.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "39\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.12772727e-01]\n",
      " [5.40637894e+01 6.26250877e+01 2.75154471e-01]\n",
      " [5.59438629e+01 6.36248779e+01 3.25059503e-01]\n",
      " [5.72167473e+01 7.17256851e+01 2.92316765e-01]\n",
      " [5.10658722e+01 6.95845032e+01 2.72055686e-01]\n",
      " [5.88233376e+01 6.49659500e+01 1.74356371e-01]\n",
      " [5.46622314e+01 6.34240799e+01 3.08985800e-01]\n",
      " [5.76356468e+01 6.89393616e+01 3.53599757e-01]\n",
      " [5.27508965e+01 7.85746460e+01 3.04907978e-01]\n",
      " [5.92619972e+01 7.51771774e+01 3.10380101e-01]\n",
      " [5.97688332e+01 7.13130341e+01 3.44596207e-01]\n",
      " [6.36136513e+01 5.36707497e+01 3.24140131e-01]\n",
      " [5.77618752e+01 5.85175934e+01 3.47171903e-01]\n",
      " [5.56946983e+01 6.17409363e+01 3.21973413e-01]\n",
      " [6.13853722e+01 7.55798264e+01 2.35569060e-01]\n",
      " [6.58748932e+01 8.38258286e+01 2.09964812e-01]\n",
      " [6.58343430e+01 7.45560532e+01 2.01042116e-01]\n",
      " [7.15199585e+01 1.02708977e+02 2.62223423e-01]\n",
      " [6.12556725e+01 7.79668045e+01 8.58783796e-02]\n",
      " [8.34827118e+01 1.12803482e+02 3.40854824e-01]\n",
      " [8.13161011e+01 9.97081299e+01 1.28811151e-01]\n",
      " [6.69658737e+01 4.57922401e+01 2.84168661e-01]\n",
      " [5.71815758e+01 7.47762756e+01 1.68776602e-01]\n",
      " [6.89337616e+01 8.04006805e+01 8.99652317e-02]\n",
      " [5.76807671e+01 9.91001434e+01 2.78182626e-01]\n",
      " [5.96790199e+01 9.50154037e+01 1.91281304e-01]\n",
      " [4.37922783e+01 3.88363686e+01 3.95607084e-01]\n",
      " [4.15727997e+01 6.34369431e+01 3.26993972e-01]\n",
      " [5.40528717e+01 3.30194321e+01 3.94513667e-01]\n",
      " [6.20521660e+01 7.91734772e+01 8.42984468e-02]\n",
      " [5.26551247e+01 9.08041687e+01 2.26555452e-01]\n",
      " [6.58873367e+01 6.33070488e+01 1.91315755e-01]\n",
      " [5.51835938e+01 7.56959763e+01 0.00000000e+00]\n",
      " [3.90684166e+01 6.72323151e+01 1.44277483e-01]\n",
      " [4.48961983e+01 8.51162338e+01 1.56872392e-01]\n",
      " [3.82692528e+01 8.95756531e+01 2.01203465e-01]\n",
      " [4.57607498e+01 4.65797234e+01 2.69121945e-01]\n",
      " [4.43602715e+01 5.20953217e+01 2.14163467e-01]\n",
      " [4.79027443e+01 7.41536255e+01 4.62318435e-02]]\n",
      "2f9f452c-bd8170e0-d4cfe79e-fbf20077-0df42d35.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "16\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 8.06282237e-02]\n",
      " [4.58410797e+01 6.04778481e+01 2.85324007e-01]\n",
      " [5.49277992e+01 5.75434456e+01 2.16920346e-01]\n",
      " [6.39735641e+01 6.48218079e+01 2.31976062e-01]\n",
      " [4.75655136e+01 7.53836441e+01 2.67593235e-01]\n",
      " [3.24992371e+01 6.38115082e+01 2.85073698e-01]\n",
      " [3.73737488e+01 5.57892876e+01 2.83660084e-01]\n",
      " [7.11444778e+01 2.88670197e+01 3.12324464e-01]\n",
      " [6.87823029e+01 4.39421616e+01 1.65154546e-01]\n",
      " [7.51998520e+01 5.82929573e+01 1.02151468e-01]\n",
      " [7.38214874e+01 1.83001080e+01 2.66732782e-01]\n",
      " [5.40102158e+01 3.00454102e+01 2.14338228e-01]\n",
      " [3.05330143e+01 5.37488441e+01 2.45681062e-01]\n",
      " [4.31968193e+01 7.65765228e+01 1.32954493e-01]\n",
      " [5.57876968e+01 8.79803772e+01 3.73678915e-02]\n",
      " [3.47833748e+01 5.50713005e+01 2.62361944e-01]]\n",
      "61bf6ce2-31d47bd9-71bb5732-53ff1d4e-ad41e3de.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "26\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.48222455e-01]\n",
      " [5.42067947e+01 6.94944687e+01 3.54865670e-01]\n",
      " [6.42807999e+01 7.28411179e+01 2.99043477e-01]\n",
      " [5.01401215e+01 7.00403900e+01 3.00962687e-01]\n",
      " [4.81043091e+01 7.36092072e+01 3.00378472e-01]\n",
      " [5.55476799e+01 7.81228333e+01 2.82621652e-01]\n",
      " [6.36730003e+01 8.12323608e+01 2.43685573e-01]\n",
      " [4.25264893e+01 7.36696854e+01 3.78855199e-01]\n",
      " [3.92143745e+01 7.31963654e+01 3.72795582e-01]\n",
      " [4.59219856e+01 7.34342346e+01 3.30490261e-01]\n",
      " [5.59119377e+01 4.44319000e+01 2.95084924e-01]\n",
      " [5.72731209e+01 5.07083626e+01 2.94900686e-01]\n",
      " [4.97735748e+01 4.27684402e+01 3.23594302e-01]\n",
      " [5.96832428e+01 4.23798714e+01 2.81416118e-01]\n",
      " [5.59960670e+01 3.74917946e+01 3.40107948e-01]\n",
      " [4.77475281e+01 4.69900551e+01 3.63263398e-01]\n",
      " [3.58395119e+01 8.13499832e+01 3.11716467e-01]\n",
      " [4.19069252e+01 8.20315399e+01 3.22596908e-01]\n",
      " [6.53125076e+01 1.03083687e+02 2.19791874e-01]\n",
      " [2.82379704e+01 9.45830307e+01 1.97867230e-01]\n",
      " [3.51786423e+01 9.36461639e+01 2.29363903e-01]\n",
      " [3.40225983e+01 9.22916870e+01 1.26018703e-01]\n",
      " [4.15361214e+01 4.76039581e+01 1.28663301e-01]\n",
      " [1.43764877e+01 4.41549377e+01 1.75577909e-01]\n",
      " [4.35350513e+00 4.18085594e+01 9.26645249e-02]\n",
      " [1.55760489e+01 9.79416733e+01 1.09768927e-01]]\n",
      "1e4d4d01-657d797c-f09f7a14-cc14e092-909e66b8.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "29\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 6.73941523e-02]\n",
      " [6.24789467e+01 6.67083817e+01 2.26090923e-01]\n",
      " [6.33412552e+01 6.70609665e+01 2.01763332e-01]\n",
      " [5.88255234e+01 7.50092239e+01 2.85295904e-01]\n",
      " [4.50103073e+01 7.41410065e+01 3.42866331e-01]\n",
      " [4.19460335e+01 7.19297180e+01 3.84925574e-01]\n",
      " [6.20649452e+01 7.70693893e+01 2.68460065e-01]\n",
      " [5.29240112e+01 5.63065834e+01 3.41916382e-01]\n",
      " [4.52511292e+01 7.39854660e+01 3.29360843e-01]\n",
      " [5.58861046e+01 8.95619736e+01 2.08401382e-01]\n",
      " [3.01248226e+01 8.14020309e+01 3.04295629e-01]\n",
      " [3.67889519e+01 9.18427353e+01 3.00656915e-01]\n",
      " [4.48896103e+01 8.24251938e+01 2.76033938e-01]\n",
      " [4.70602341e+01 5.82184448e+01 2.62159944e-01]\n",
      " [5.75106926e+01 3.92824173e+01 3.30651343e-01]\n",
      " [6.30911636e+01 4.72563095e+01 2.11433619e-01]\n",
      " [4.07519264e+01 4.22297363e+01 3.54950905e-01]\n",
      " [6.56293869e+01 6.45919037e+01 1.23839766e-01]\n",
      " [4.63984680e+01 4.10570183e+01 2.53059298e-01]\n",
      " [4.46994629e+01 6.56534958e+01 1.85841888e-01]\n",
      " [3.82778816e+01 7.53883209e+01 2.39845425e-01]\n",
      " [3.15704098e+01 9.93653030e+01 1.87908530e-01]\n",
      " [7.20268631e+01 9.52458649e+01 8.26014206e-02]\n",
      " [5.92224922e+01 1.16456596e+02 5.51161915e-02]\n",
      " [6.54570084e+01 8.94373550e+01 5.18523641e-02]\n",
      " [8.29126053e+01 6.76834869e+01 1.19531512e-01]\n",
      " [6.56217270e+01 3.43191147e+01 3.31268132e-01]\n",
      " [5.02748146e+01 4.15655785e+01 1.98911265e-01]\n",
      " [6.36474571e+01 6.42743225e+01 1.68291137e-01]]\n",
      "107fb23d-4d6f914d-fd7d0307-5198c481-588d7dd5.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "31\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 8.41433555e-02]\n",
      " [5.32863922e+01 5.86567879e+01 3.33901554e-01]\n",
      " [6.55893936e+01 7.12077942e+01 2.82227755e-01]\n",
      " [7.74502640e+01 3.70270424e+01 1.79430738e-01]\n",
      " [8.05059128e+01 9.30023861e+00 3.68602633e-01]\n",
      " [5.83856392e+01 4.30751228e+01 3.56470197e-01]\n",
      " [5.78246689e+01 4.55263634e+01 2.77184308e-01]\n",
      " [5.50564766e+01 4.41059761e+01 3.38516444e-01]\n",
      " [5.70196495e+01 4.07842751e+01 3.73176128e-01]\n",
      " [7.01896057e+01 6.10900536e+01 3.92084241e-01]\n",
      " [7.88775558e+01 9.26547699e+01 3.93840700e-01]\n",
      " [7.00437546e+01 6.87051773e+01 4.16755408e-01]\n",
      " [7.77540894e+01 7.55699310e+01 3.99544418e-01]\n",
      " [7.70431519e+01 6.45179596e+01 2.66094893e-01]\n",
      " [8.16210327e+01 7.82863159e+01 4.08632010e-01]\n",
      " [7.93458786e+01 9.36311722e+01 4.00066704e-01]\n",
      " [8.53776169e+01 8.37931290e+01 3.60068381e-01]\n",
      " [8.08723831e+01 1.02092819e+02 4.58632678e-01]\n",
      " [5.65503197e+01 9.84036865e+01 3.60191911e-01]\n",
      " [5.87537384e+01 1.11096069e+02 3.04017395e-01]\n",
      " [7.11882324e+01 1.08744583e+02 3.87984335e-01]\n",
      " [5.55004463e+01 7.96476517e+01 4.98243302e-01]\n",
      " [5.20597343e+01 6.05802956e+01 3.72761309e-01]\n",
      " [4.23854713e+01 5.12450638e+01 3.57451886e-01]\n",
      " [2.67126274e+01 7.46771774e+01 4.28904623e-01]\n",
      " [3.78087845e+01 9.02105484e+01 3.04343849e-01]\n",
      " [5.82968407e+01 1.04908234e+02 3.33643347e-01]\n",
      " [3.83608131e+01 4.77162743e+01 3.68413359e-01]\n",
      " [4.83055725e+01 4.10711060e+01 3.51061732e-01]\n",
      " [6.20744057e+01 4.54896088e+01 3.94925445e-01]\n",
      " [6.63451080e+01 4.96545525e+01 4.01372969e-01]]\n",
      "911b4d3f-678028ff-112f9137-5584d665-6dfed8e5.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "27\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 0.00000000e+00]\n",
      " [6.10781441e+01 7.33544312e+01 1.85091957e-01]\n",
      " [6.97169952e+01 7.14017792e+01 1.29670292e-01]\n",
      " [5.86014824e+01 8.66852112e+01 1.19192854e-01]\n",
      " [5.66362038e+01 7.84805374e+01 1.76286057e-01]\n",
      " [6.07085762e+01 7.88001938e+01 1.71746880e-01]\n",
      " [7.39226990e+01 8.09553452e+01 1.01887539e-01]\n",
      " [5.86466866e+01 7.96189041e+01 2.08792567e-01]\n",
      " [5.42751923e+01 8.81447754e+01 2.52430975e-01]\n",
      " [6.54256287e+01 7.76374130e+01 7.85451084e-02]\n",
      " [7.55497360e+01 7.50234756e+01 1.11792997e-01]\n",
      " [6.54496918e+01 8.47514420e+01 1.20510578e-01]\n",
      " [4.63475876e+01 7.90913467e+01 2.94089049e-01]\n",
      " [6.29972496e+01 7.68253479e+01 1.74877107e-01]\n",
      " [6.87025223e+01 9.22394409e+01 1.12052292e-01]\n",
      " [5.33376770e+01 8.40874557e+01 2.75229990e-01]\n",
      " [5.00333099e+01 8.19925308e+01 3.03940177e-01]\n",
      " [5.82738609e+01 9.71420212e+01 2.14991987e-01]\n",
      " [7.39520493e+01 5.19647675e+01 7.51405731e-02]\n",
      " [6.17936363e+01 3.09360600e+01 3.21659982e-01]\n",
      " [5.03252678e+01 2.66904411e+01 3.24855238e-01]\n",
      " [4.38219566e+01 2.91807518e+01 2.92612076e-01]\n",
      " [5.54696236e+01 1.02281075e+02 2.47803479e-02]\n",
      " [5.48355064e+01 1.03589500e+02 1.19163893e-01]\n",
      " [3.18089256e+01 1.09543221e+02 2.06478626e-01]\n",
      " [5.11048050e+01 9.25388489e+01 2.02415153e-01]\n",
      " [8.06168365e+01 9.59491425e+01 0.00000000e+00]]\n",
      "a2745b2a-00401cf6-e64c5491-7ed57ef3-d416dc0f.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "46\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 8.33702236e-02]\n",
      " [4.12247620e+01 6.65855026e+01 3.05201024e-01]\n",
      " [4.48049850e+01 7.42833481e+01 3.15436184e-01]\n",
      " [4.97692795e+01 6.68244781e+01 3.39473337e-01]\n",
      " [4.27709427e+01 8.41694946e+01 3.61522466e-01]\n",
      " [6.44634552e+01 1.07672478e+02 3.37499082e-01]\n",
      " [7.79386444e+01 7.07207184e+01 3.04339617e-01]\n",
      " [6.88555679e+01 7.16995087e+01 3.64456594e-01]\n",
      " [4.09229507e+01 7.68570557e+01 3.71660650e-01]\n",
      " [6.70395203e+01 7.21004791e+01 3.61514837e-01]\n",
      " [9.60924301e+01 7.79789429e+01 2.97857016e-01]\n",
      " [8.78432922e+01 6.68939285e+01 3.17799360e-01]\n",
      " [8.94487762e+01 5.76550827e+01 3.51794630e-01]\n",
      " [9.59329300e+01 8.55365601e+01 2.76915759e-01]\n",
      " [8.82168579e+01 7.41981964e+01 3.27546328e-01]\n",
      " [7.33749542e+01 8.01564407e+01 2.91854322e-01]\n",
      " [7.89042664e+01 7.58830185e+01 1.79193184e-01]\n",
      " [7.94876709e+01 7.58306046e+01 2.65646964e-01]\n",
      " [4.56935425e+01 7.09015656e+01 3.24122757e-01]\n",
      " [4.43564415e+01 8.17941895e+01 3.01961869e-01]\n",
      " [6.33356552e+01 1.09771805e+02 3.08507502e-01]\n",
      " [5.27887840e+01 7.05358505e+01 4.51103866e-01]\n",
      " [6.31873589e+01 8.48690491e+01 4.35132951e-01]\n",
      " [6.72137604e+01 8.86952133e+01 3.49763006e-01]\n",
      " [7.47368240e+01 8.50079651e+01 2.59231985e-01]\n",
      " [9.22497177e+01 9.95386353e+01 3.18779171e-01]\n",
      " [6.68330765e+01 1.06748512e+02 2.94806093e-01]\n",
      " [6.62117386e+01 9.75992737e+01 2.62516320e-01]\n",
      " [6.48505478e+01 8.61825409e+01 1.80748552e-01]\n",
      " [6.55422134e+01 6.84305420e+01 1.60966545e-01]\n",
      " [7.51914597e+01 7.52143784e+01 2.27642179e-01]\n",
      " [6.73822021e+01 6.90725098e+01 5.16923005e-03]\n",
      " [7.18448105e+01 5.57237854e+01 2.78929472e-01]\n",
      " [7.59546356e+01 1.14164482e+02 3.07608426e-01]\n",
      " [7.21902466e+01 7.11193390e+01 0.00000000e+00]\n",
      " [9.58252335e+01 3.60868835e+01 2.26570636e-01]\n",
      " [8.02109604e+01 3.96180038e+01 2.46007919e-01]\n",
      " [5.70810738e+01 4.19703636e+01 3.21276516e-01]\n",
      " [4.99664040e+01 9.83473740e+01 2.53428310e-01]\n",
      " [3.20664330e+01 5.50925980e+01 3.02071214e-01]\n",
      " [2.76025562e+01 5.61721115e+01 3.45123798e-01]\n",
      " [3.44130211e+01 4.79116554e+01 3.93640876e-01]\n",
      " [2.78032761e+01 4.86207085e+01 3.75972122e-01]\n",
      " [7.06733551e+01 5.72332306e+01 2.57620484e-01]\n",
      " [9.16360245e+01 7.12681046e+01 2.40329728e-01]\n",
      " [1.06892700e+02 4.98137054e+01 9.99224186e-02]]\n",
      "fcb5a910-58f79c1f-460b0691-593caa44-0c367fb7.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "17\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.19956399e-04]\n",
      " [4.16902542e+01 6.50107040e+01 1.67849660e-01]\n",
      " [3.73691177e+01 5.02026863e+01 2.69799232e-01]\n",
      " [4.55615425e+01 6.29797058e+01 2.67795295e-01]\n",
      " [2.57352066e+01 6.65259705e+01 2.93462634e-01]\n",
      " [2.51988392e+01 6.28659058e+01 3.09805602e-01]\n",
      " [2.52697411e+01 5.49564705e+01 3.06649834e-01]\n",
      " [3.87292938e+01 5.62496338e+01 3.37074488e-01]\n",
      " [4.97034416e+01 1.02996765e+02 9.91907045e-02]\n",
      " [7.89504623e+01 1.10109680e+02 9.50569138e-02]\n",
      " [7.88198471e+01 9.34451752e+01 2.34902591e-01]\n",
      " [7.42179489e+01 2.28188534e+01 3.11223477e-01]\n",
      " [4.30160637e+01 2.90567398e+01 2.83093452e-01]\n",
      " [2.42290573e+01 4.39600754e+01 2.55900323e-01]\n",
      " [1.45321026e+01 4.46670418e+01 2.59834588e-01]\n",
      " [2.00184822e+01 5.89310875e+01 2.75142878e-01]\n",
      " [7.12146454e+01 6.23234596e+01 0.00000000e+00]]\n",
      "434c6923-7fd109e9-4b3f4c2c-ef699856-449423a0.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "50\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.19760031e-01]\n",
      " [5.29311218e+01 6.87824554e+01 2.48091549e-01]\n",
      " [3.85298309e+01 7.79071960e+01 2.93684781e-01]\n",
      " [2.81357136e+01 5.84556160e+01 3.37918013e-01]\n",
      " [2.58113346e+01 7.20956116e+01 3.65299493e-01]\n",
      " [4.19375534e+01 7.75676804e+01 3.64268273e-01]\n",
      " [4.49533386e+01 6.31355286e+01 3.46223503e-01]\n",
      " [4.95619354e+01 7.55708542e+01 3.08158189e-01]\n",
      " [2.83777199e+01 5.90275993e+01 3.62660348e-01]\n",
      " [4.56051750e+01 6.08277664e+01 3.36649954e-01]\n",
      " [4.18059921e+01 7.20257568e+01 3.33388746e-01]\n",
      " [5.39065895e+01 7.25720596e+01 2.95336276e-01]\n",
      " [6.39916801e+01 5.71435890e+01 3.07240784e-01]\n",
      " [5.97832222e+01 5.24385796e+01 2.89428413e-01]\n",
      " [4.96782990e+01 8.54325027e+01 1.82087436e-01]\n",
      " [7.36882935e+01 4.33266182e+01 2.59208351e-01]\n",
      " [4.79891586e+01 7.25788345e+01 3.30679089e-01]\n",
      " [5.64311790e+01 5.48974991e+01 2.65032470e-01]\n",
      " [4.94560051e+01 4.63540497e+01 2.73262531e-01]\n",
      " [4.05391884e+01 5.78750877e+01 2.67490953e-01]\n",
      " [6.31310539e+01 6.91613007e+01 2.04406723e-01]\n",
      " [6.71355362e+01 3.47640190e+01 4.43219841e-01]\n",
      " [6.12366905e+01 4.96243706e+01 2.92671800e-01]\n",
      " [7.81525803e+01 1.10719658e+02 1.79994166e-01]\n",
      " [8.07905045e+01 3.24817047e+01 3.75515610e-01]\n",
      " [6.39794693e+01 3.49004745e+01 3.43353212e-01]\n",
      " [5.14325256e+01 4.59980125e+01 3.92198205e-01]\n",
      " [5.46008682e+01 3.29080696e+01 4.62915897e-01]\n",
      " [5.78903046e+01 3.68636017e+01 3.66732299e-01]\n",
      " [5.91627197e+01 6.35825462e+01 2.11619824e-01]\n",
      " [5.87360039e+01 3.93140984e+01 3.58120531e-01]\n",
      " [6.15832100e+01 3.55010605e+01 3.63819957e-01]\n",
      " [5.91102448e+01 4.41618500e+01 2.97993928e-01]\n",
      " [6.17426682e+01 4.21580734e+01 2.68398464e-01]\n",
      " [5.37878952e+01 5.12099266e+01 2.73010910e-01]\n",
      " [5.09961777e+01 4.51694489e+01 3.31679523e-01]\n",
      " [6.00386086e+01 3.96186905e+01 1.98821232e-01]\n",
      " [7.27092056e+01 2.89226723e+01 3.55404615e-01]\n",
      " [9.01322327e+01 2.10468769e+01 3.64931136e-01]\n",
      " [7.41965942e+01 4.35503922e+01 1.39621884e-01]\n",
      " [7.19976120e+01 4.18591042e+01 1.84926212e-01]\n",
      " [6.00983047e+01 4.02229195e+01 9.56149846e-02]\n",
      " [6.11273918e+01 3.47947273e+01 2.71483481e-01]\n",
      " [5.70527611e+01 6.91893311e+01 4.48678397e-02]\n",
      " [8.28742828e+01 4.40307541e+01 1.52344137e-01]\n",
      " [7.58188629e+01 3.18200970e+01 3.33057821e-01]\n",
      " [5.05682640e+01 4.53328171e+01 2.48463273e-01]\n",
      " [7.04935608e+01 4.17807388e+01 2.12419137e-01]\n",
      " [5.58032913e+01 6.07810974e+01 1.31543502e-01]\n",
      " [5.02623520e+01 6.02805214e+01 1.77711308e-01]]\n",
      "563b4fb8-f47e2609-e2619e81-cbb68278-ff457903.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "46\n",
      "##########################################\n",
      "[[ 64.          64.           0.28190592]\n",
      " [ 51.21274567  56.76581192   0.32005122]\n",
      " [ 64.74060059  64.13343811   0.30885512]\n",
      " [ 45.63141251  63.49371719   0.34756994]\n",
      " [ 52.40908432  65.02861023   0.35657021]\n",
      " [ 33.88572311  73.25616455   0.35519108]\n",
      " [ 45.44991302  64.67371368   0.434001  ]\n",
      " [ 56.75463486  65.78344727   0.34445783]\n",
      " [ 56.18663406  63.05580902   0.34553754]\n",
      " [ 62.33299255  52.57022095   0.33256367]\n",
      " [ 61.0989151   57.50133133   0.35589764]\n",
      " [ 50.365448    60.58365631   0.39513451]\n",
      " [ 64.53244781  66.16398621   0.33928677]\n",
      " [ 63.44296265  69.2795105    0.29108632]\n",
      " [ 66.62789917  61.25708771   0.25921226]\n",
      " [ 54.6686058   63.33311844   0.24438755]\n",
      " [ 49.70167923  43.86263657   0.35015857]\n",
      " [ 67.8952713   58.25138474   0.24088858]\n",
      " [ 60.2725029   85.04625702   0.34105957]\n",
      " [ 50.10692215  94.58620453   0.33212548]\n",
      " [ 49.56001282 103.72293091   0.27585587]\n",
      " [ 68.013237    95.58999634   0.33408964]\n",
      " [ 41.74156952  71.23963928   0.33206668]\n",
      " [ 48.24367523  64.48083496   0.24726912]\n",
      " [ 49.90699768  60.24768066   0.22606052]\n",
      " [ 51.03652191  53.98648453   0.30979225]\n",
      " [ 62.28516006  85.52307892   0.31800097]\n",
      " [ 63.99806213  60.68581009   0.16437089]\n",
      " [ 55.58211899  59.2266922    0.22494259]\n",
      " [ 57.16423798  82.68360138   0.2513237 ]\n",
      " [ 70.25646973  85.16243744   0.21546753]\n",
      " [ 59.87926865  45.58126068   0.2168559 ]\n",
      " [ 67.61009979  39.41464996   0.31609139]\n",
      " [ 54.31652832  77.74626923   0.27127773]\n",
      " [ 52.77829742  69.02552032   0.2699897 ]\n",
      " [ 46.59451294  74.20493317   0.32089046]\n",
      " [ 44.22431183  69.3717804    0.35740763]\n",
      " [ 49.62995911  54.47202301   0.22727817]\n",
      " [ 39.03877258  71.17627716   0.27677628]\n",
      " [ 46.85816956  63.76096344   0.12446196]\n",
      " [ 60.11938095  32.27085495   0.31377667]\n",
      " [ 55.54774475  34.47058105   0.37994608]\n",
      " [ 37.8053894   76.0819931    0.32355252]\n",
      " [ 37.86675262  64.00416565   0.28260651]\n",
      " [ 49.40761948  30.68654633   0.30782539]\n",
      " [ 73.62250519  24.82912064   0.28563556]]\n",
      "7eaa380d-cc85a446-6b0ae8ec-84695181-190f2cbb.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "32\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 7.76871294e-02]\n",
      " [5.26714821e+01 6.33273544e+01 2.45921835e-01]\n",
      " [6.42112503e+01 6.40981369e+01 1.84655368e-01]\n",
      " [6.53453522e+01 8.24369354e+01 1.89879417e-01]\n",
      " [6.72034531e+01 8.11568985e+01 1.08213536e-01]\n",
      " [6.08271599e+01 6.15573616e+01 2.70063639e-01]\n",
      " [5.75933228e+01 5.42170982e+01 3.26865494e-01]\n",
      " [5.48067932e+01 7.52518997e+01 2.61759311e-01]\n",
      " [5.55846252e+01 5.36150246e+01 2.72434145e-01]\n",
      " [4.79753876e+01 8.42339706e+01 2.41940334e-01]\n",
      " [6.98673019e+01 8.75461426e+01 2.22991079e-01]\n",
      " [5.83079605e+01 4.11100922e+01 3.59538674e-01]\n",
      " [6.18204155e+01 4.22924118e+01 2.67285913e-01]\n",
      " [5.05441208e+01 4.93945618e+01 2.83818394e-01]\n",
      " [5.95163689e+01 4.25816193e+01 3.06805521e-01]\n",
      " [6.68750763e+01 5.02450981e+01 2.21759468e-01]\n",
      " [7.52122574e+01 8.37427063e+01 7.25275353e-02]\n",
      " [7.03048553e+01 4.55744705e+01 8.13322514e-02]\n",
      " [6.73634262e+01 7.19791718e+01 3.65727916e-02]\n",
      " [7.29853516e+01 1.00279671e+02 1.66969612e-01]\n",
      " [8.03307419e+01 8.23479843e+01 1.53557852e-01]\n",
      " [7.12656479e+01 5.16080017e+01 2.08470196e-01]\n",
      " [7.72775421e+01 6.17670631e+01 1.15523607e-01]\n",
      " [7.06540909e+01 4.04312439e+01 2.92489409e-01]\n",
      " [6.16117630e+01 4.62417526e+01 3.15063477e-01]\n",
      " [7.04056320e+01 4.72062073e+01 1.63755417e-01]\n",
      " [6.18552132e+01 9.86347733e+01 9.52155441e-02]\n",
      " [5.87067566e+01 4.85724983e+01 2.10380763e-01]\n",
      " [6.43333740e+01 4.12356911e+01 3.36424917e-01]\n",
      " [5.82392311e+01 7.82429962e+01 1.38820097e-01]\n",
      " [6.13215790e+01 4.13913536e+01 3.19480747e-01]\n",
      " [6.13276825e+01 4.83117599e+01 2.51540750e-01]]\n",
      "d9fd52c8-f9e142f5-8d8e4431-e8c4981c-bf4c21ca.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "27\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 0.00000000e+00]\n",
      " [5.32636223e+01 6.75751419e+01 2.26651832e-01]\n",
      " [7.50111389e+01 7.05324631e+01 1.72637969e-01]\n",
      " [7.55506897e+01 5.25245934e+01 3.24400336e-01]\n",
      " [7.21630249e+01 4.55637512e+01 2.82861561e-01]\n",
      " [6.24308853e+01 8.55236282e+01 3.37970197e-01]\n",
      " [6.80513458e+01 4.73959084e+01 3.09179783e-01]\n",
      " [6.96943359e+01 5.80500069e+01 2.47303158e-01]\n",
      " [5.58844795e+01 8.48563538e+01 2.66914546e-01]\n",
      " [5.58691521e+01 7.87366104e+01 3.12435925e-01]\n",
      " [7.38538818e+01 7.59507294e+01 2.25634366e-01]\n",
      " [7.05466156e+01 6.30081673e+01 3.13135207e-01]\n",
      " [6.70049667e+01 7.10645905e+01 2.97905952e-01]\n",
      " [6.50875092e+01 4.60796318e+01 3.44325274e-01]\n",
      " [5.96001701e+01 7.32460861e+01 2.79364854e-01]\n",
      " [9.02387543e+01 9.82450409e+01 9.12296325e-02]\n",
      " [8.52234192e+01 9.53054199e+01 1.32359579e-01]\n",
      " [8.39156570e+01 5.94263954e+01 9.36267525e-02]\n",
      " [6.49061813e+01 5.24183426e+01 2.16601357e-01]\n",
      " [4.84462128e+01 9.69137192e+01 2.68774211e-01]\n",
      " [8.69759140e+01 1.10764206e+02 2.07587838e-01]\n",
      " [5.01818466e+01 8.86225739e+01 1.80075675e-01]\n",
      " [3.07325039e+01 7.92447662e+01 1.56664521e-01]\n",
      " [4.06540947e+01 9.59456177e+01 5.66058159e-02]\n",
      " [7.85892410e+01 3.82503395e+01 1.00105435e-01]\n",
      " [9.12900848e+01 6.83268585e+01 0.00000000e+00]\n",
      " [9.92076035e+01 9.19402008e+01 0.00000000e+00]]\n",
      "89fa5b3e-b7397022-15284035-4dfd236e-4feb14f1.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "25\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.37502313e-01]\n",
      " [4.96190834e+01 7.03343506e+01 2.32741252e-01]\n",
      " [6.25197487e+01 6.81708527e+01 2.24787802e-01]\n",
      " [8.35739899e+01 6.87726669e+01 1.30908847e-01]\n",
      " [8.11548386e+01 5.74230080e+01 3.01229656e-02]\n",
      " [6.64337540e+01 6.08969803e+01 1.61011517e-01]\n",
      " [8.42275696e+01 7.73433151e+01 5.09195477e-02]\n",
      " [7.09779587e+01 6.58064651e+01 2.28834391e-01]\n",
      " [4.59630814e+01 7.18266525e+01 3.36019754e-01]\n",
      " [4.94004250e+01 7.19380722e+01 3.16290140e-01]\n",
      " [6.34102020e+01 7.10376205e+01 2.26326466e-01]\n",
      " [7.86698608e+01 6.51092529e+01 2.90171206e-01]\n",
      " [7.72185745e+01 5.18078537e+01 1.39021739e-01]\n",
      " [7.37953033e+01 3.71155357e+01 2.61826426e-01]\n",
      " [8.51472626e+01 9.03866959e+01 0.00000000e+00]\n",
      " [1.01066124e+02 1.03910797e+02 0.00000000e+00]\n",
      " [1.01152809e+02 1.09630829e+02 8.47665742e-02]\n",
      " [8.57878647e+01 6.03891106e+01 0.00000000e+00]\n",
      " [9.76494827e+01 6.42478943e+01 0.00000000e+00]\n",
      " [9.51200943e+01 7.31751328e+01 0.00000000e+00]\n",
      " [9.38815002e+01 3.00762730e+01 5.55004664e-02]\n",
      " [9.53842392e+01 7.19200134e+01 0.00000000e+00]\n",
      " [5.38167572e+01 7.11433334e+01 7.49810264e-02]\n",
      " [5.11994171e+01 6.38102951e+01 9.43657607e-02]\n",
      " [4.44616699e+01 9.79836044e+01 5.97308204e-02]]\n",
      "a439aa5a-bb7de25c-957b50e6-84b63133-744396b1.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "19\n",
      "##########################################\n",
      "[[64.         64.          0.        ]\n",
      " [43.82347488 61.47084427  0.2774753 ]\n",
      " [47.1962738  71.95120239  0.29462475]\n",
      " [44.90581131 68.7703476   0.29820156]\n",
      " [51.70294571 66.22579193  0.18557923]\n",
      " [44.6737175  74.0959549   0.2368812 ]\n",
      " [43.93806839 69.80315399  0.35495624]\n",
      " [50.69109344 72.0538559   0.2373189 ]\n",
      " [47.03936768 74.85105133  0.3355152 ]\n",
      " [53.36208725 67.04664612  0.31209719]\n",
      " [50.90938187 74.52929688  0.26454151]\n",
      " [58.42957306 86.19936371  0.30539674]\n",
      " [57.61719894 55.32743454  0.31088248]\n",
      " [53.63232422 46.98478699  0.28570861]\n",
      " [39.72460938 58.04421616  0.27682179]\n",
      " [41.66081238 93.0368042   0.27845377]\n",
      " [38.60939789 93.48402405  0.3272213 ]\n",
      " [61.59003448 66.73625946  0.18289587]\n",
      " [71.08148956 50.76590729  0.21122029]]\n",
      "abb11c17-b8d815cf-fba388b3-0ee5a930-b53ad1a2.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "31\n",
      "##########################################\n",
      "[[ 64.          64.           0.21087627]\n",
      " [ 44.06234741  65.2878418    0.28381267]\n",
      " [ 43.64242935  68.134552     0.30130219]\n",
      " [ 36.49710464  70.49621582   0.31075522]\n",
      " [ 35.83687592  66.00442505   0.34310511]\n",
      " [ 29.57804489  71.34606171   0.33915517]\n",
      " [ 52.3121376   67.6836853    0.26546597]\n",
      " [ 57.97600937  69.05673218   0.2966769 ]\n",
      " [ 37.90728378  69.67932892   0.35935855]\n",
      " [ 42.72787476  68.35476685   0.38344854]\n",
      " [ 45.63716507  68.67259216   0.31944272]\n",
      " [ 40.22665787  71.48729706   0.3830145 ]\n",
      " [ 36.65053558  64.56673431   0.3233656 ]\n",
      " [ 36.88568878  72.08569336   0.33283368]\n",
      " [ 39.53850555  80.05846405   0.31204468]\n",
      " [ 50.43943024  84.6679306    0.33100495]\n",
      " [ 48.03282166  88.72589874   0.2312548 ]\n",
      " [ 55.55192184  53.09770203   0.29903713]\n",
      " [ 36.40699005  45.26799393   0.38795838]\n",
      " [ 38.74583435  45.25928116   0.37323833]\n",
      " [ 36.59288025  46.80319595   0.40285853]\n",
      " [ 39.41078949  40.17803955   0.4020254 ]\n",
      " [ 41.26803207  57.29422379   0.32848972]\n",
      " [ 59.77934265  37.24789429   0.30889073]\n",
      " [ 42.72280884  52.59152985   0.46158561]\n",
      " [ 35.93165588 100.68293762   0.30961826]\n",
      " [ 34.0712204  100.94644928   0.28323022]\n",
      " [ 34.39092255  93.24954987   0.30010846]\n",
      " [ 25.60479355  73.66247559   0.30183256]\n",
      " [ 29.31053925  77.47332764   0.34366208]\n",
      " [ 44.04880142  67.69995117   0.28687823]]\n",
      "c3a8a376-a4621465-bb65ba5d-d8dd3daa-e3e005eb.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "22\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.43886641e-01]\n",
      " [6.51328506e+01 6.62544250e+01 2.51593828e-01]\n",
      " [6.85689850e+01 7.63099747e+01 9.29713920e-02]\n",
      " [6.72288132e+01 7.43151855e+01 2.33196035e-01]\n",
      " [6.57370682e+01 8.45256500e+01 2.15039119e-01]\n",
      " [4.59831886e+01 7.08801193e+01 3.29564363e-01]\n",
      " [5.19502373e+01 7.18893585e+01 1.96913347e-01]\n",
      " [4.55832825e+01 6.06033592e+01 2.25284949e-01]\n",
      " [5.30326767e+01 6.00442200e+01 2.73542732e-01]\n",
      " [6.87238770e+01 4.52463417e+01 2.25746796e-01]\n",
      " [3.21770134e+01 7.10771484e+01 3.18539232e-01]\n",
      " [3.47382774e+01 7.83542099e+01 3.02704424e-01]\n",
      " [5.65283279e+01 8.00535889e+01 1.06546305e-01]\n",
      " [5.63928070e+01 6.12680855e+01 1.62855640e-01]\n",
      " [6.34402466e+01 7.86986160e+01 1.37731403e-01]\n",
      " [7.13181076e+01 6.30718956e+01 2.20464066e-01]\n",
      " [7.27939758e+01 4.03510704e+01 1.80710599e-01]\n",
      " [7.80496368e+01 2.85596905e+01 2.10275784e-01]\n",
      " [7.42652969e+01 3.69428215e+01 7.76594877e-02]\n",
      " [7.35778809e+01 7.26509476e+01 3.28566246e-02]\n",
      " [5.72784462e+01 1.03800842e+02 2.28077948e-01]\n",
      " [6.07240295e+01 1.11809425e+02 2.81035099e-02]]\n",
      "e5f5c090-09b7a708-82fcdee4-9f869d74-ca3d40a8.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "39\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 9.86769050e-03]\n",
      " [5.71069298e+01 6.71254807e+01 1.99404836e-01]\n",
      " [6.41768875e+01 6.58832016e+01 1.61068290e-01]\n",
      " [6.52038803e+01 7.13265839e+01 1.18555881e-01]\n",
      " [5.71145058e+01 8.09319305e+01 2.08024681e-01]\n",
      " [4.37882614e+01 7.74563370e+01 2.11221471e-01]\n",
      " [4.82422523e+01 7.21199036e+01 2.10574642e-01]\n",
      " [5.77401161e+01 7.98211975e+01 3.55061479e-02]\n",
      " [6.68701248e+01 1.03957603e+02 1.65017992e-01]\n",
      " [7.45813141e+01 1.07354836e+02 1.56081364e-01]\n",
      " [6.17554703e+01 8.16118774e+01 1.68325067e-01]\n",
      " [6.55932541e+01 7.84218903e+01 1.66967303e-01]\n",
      " [7.16942749e+01 6.92272263e+01 1.86090320e-01]\n",
      " [4.31488457e+01 8.66884003e+01 2.85241842e-01]\n",
      " [5.04405365e+01 8.60738220e+01 8.56860355e-02]\n",
      " [7.13085861e+01 8.17603912e+01 6.66131377e-02]\n",
      " [7.00708466e+01 7.91929169e+01 1.12420268e-01]\n",
      " [7.35605621e+01 7.91963348e+01 1.66411489e-01]\n",
      " [6.00007782e+01 9.47973175e+01 2.13318169e-01]\n",
      " [5.63218460e+01 8.94214859e+01 2.23657027e-01]\n",
      " [6.31829681e+01 8.58085938e+01 1.78902581e-01]\n",
      " [6.44008255e+01 8.64723129e+01 2.57947177e-01]\n",
      " [4.41889725e+01 8.05392303e+01 1.31079718e-01]\n",
      " [4.23708000e+01 7.93770828e+01 1.27692878e-01]\n",
      " [3.87562141e+01 5.21280327e+01 3.18617970e-01]\n",
      " [3.78349495e+01 5.77127571e+01 3.55075896e-01]\n",
      " [5.67202644e+01 4.43996429e+01 3.65403682e-01]\n",
      " [5.56965332e+01 3.96119232e+01 3.56375247e-01]\n",
      " [5.56346512e+01 4.22050629e+01 2.95439303e-01]\n",
      " [6.53548203e+01 7.20877075e+01 1.48262635e-01]\n",
      " [5.73901405e+01 9.45874863e+01 1.07979804e-01]\n",
      " [6.12730370e+01 1.00673790e+02 0.00000000e+00]\n",
      " [6.05746269e+01 6.63141708e+01 0.00000000e+00]\n",
      " [4.10438881e+01 9.74877014e+01 3.90065163e-02]\n",
      " [5.59916115e+01 8.71649399e+01 1.06216036e-02]\n",
      " [5.43774223e+01 4.13234596e+01 2.94269443e-01]\n",
      " [6.03892708e+01 5.39196014e+01 1.84741765e-01]\n",
      " [3.61440544e+01 8.38851852e+01 2.16213003e-01]\n",
      " [4.42507133e+01 9.32317276e+01 2.59679407e-01]]\n",
      "727f555b-ca31baa2-5a5d16fd-ca9b8960-5a9ce4e0.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "45\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.16005063e-01]\n",
      " [5.29398499e+01 6.91453018e+01 2.49030128e-01]\n",
      " [7.50983124e+01 6.43591461e+01 1.90252274e-01]\n",
      " [6.21550179e+01 7.22052536e+01 2.45800570e-01]\n",
      " [6.37896271e+01 7.55137787e+01 2.34860346e-01]\n",
      " [5.62743111e+01 6.97466354e+01 2.93154866e-01]\n",
      " [4.37753944e+01 7.12252655e+01 3.31448823e-01]\n",
      " [4.96283150e+01 6.99603043e+01 3.06238085e-01]\n",
      " [5.48192520e+01 6.83201599e+01 2.66850650e-01]\n",
      " [5.44005737e+01 7.16211243e+01 2.79276669e-01]\n",
      " [5.07676926e+01 8.60503922e+01 2.13500097e-01]\n",
      " [5.47980690e+01 8.04130402e+01 2.13140026e-01]\n",
      " [7.71498337e+01 6.83866959e+01 8.02574828e-02]\n",
      " [7.98693161e+01 3.93013535e+01 2.30623588e-01]\n",
      " [5.55076408e+01 8.83610992e+01 8.96190554e-02]\n",
      " [6.63059464e+01 9.56969757e+01 1.31856501e-01]\n",
      " [4.99213333e+01 9.23223724e+01 2.63074964e-01]\n",
      " [5.97869453e+01 6.82268600e+01 7.81957284e-02]\n",
      " [7.07912445e+01 8.27887497e+01 0.00000000e+00]\n",
      " [6.41084595e+01 9.24040298e+01 2.53218636e-02]\n",
      " [6.10679092e+01 8.02122574e+01 7.46361315e-02]\n",
      " [8.31955795e+01 4.92400169e+01 1.72993630e-01]\n",
      " [7.24698257e+01 4.83901634e+01 2.20614910e-01]\n",
      " [7.20678635e+01 5.88660622e+01 0.00000000e+00]\n",
      " [5.96283531e+01 3.72490463e+01 3.23046207e-01]\n",
      " [6.57181015e+01 3.97890739e+01 2.67029494e-01]\n",
      " [5.88471489e+01 9.10960388e+01 8.73935744e-02]\n",
      " [9.14563980e+01 1.02165398e+02 9.81152132e-02]\n",
      " [8.54559937e+01 4.19614944e+01 1.61272213e-01]\n",
      " [6.58965759e+01 6.92162704e+01 8.23628306e-02]\n",
      " [7.29112549e+01 3.72444420e+01 1.74408853e-01]\n",
      " [8.63387451e+01 3.29805107e+01 2.98201889e-01]\n",
      " [7.00654755e+01 3.21677094e+01 3.37612748e-01]\n",
      " [8.14894485e+01 7.75352173e+01 0.00000000e+00]\n",
      " [7.07981339e+01 1.04349968e+02 0.00000000e+00]\n",
      " [5.33736191e+01 8.68663330e+01 0.00000000e+00]\n",
      " [5.78599205e+01 1.00639000e+02 7.93748796e-02]\n",
      " [6.10996513e+01 6.17205162e+01 6.19827099e-02]\n",
      " [6.66920471e+01 3.78398170e+01 3.38792562e-01]\n",
      " [7.50877838e+01 8.70374832e+01 4.13439125e-02]\n",
      " [8.61323700e+01 9.07179565e+01 0.00000000e+00]\n",
      " [7.81741104e+01 3.20160599e+01 2.52963096e-01]\n",
      " [8.09161758e+01 3.14877052e+01 2.66025335e-01]\n",
      " [7.58674774e+01 3.90504494e+01 2.45104119e-01]\n",
      " [7.10092850e+01 3.22618179e+01 2.32628837e-01]]\n",
      "b1dc00c1-529b01aa-15739ab5-54f2d242-71c227db.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "39\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 6.43518642e-02]\n",
      " [6.88429031e+01 6.44541931e+01 1.80457875e-01]\n",
      " [6.67902298e+01 6.20200996e+01 8.43827352e-02]\n",
      " [6.08525887e+01 6.25791931e+01 0.00000000e+00]\n",
      " [6.00712967e+01 4.92928047e+01 2.59966731e-01]\n",
      " [6.39521980e+01 9.96813736e+01 0.00000000e+00]\n",
      " [5.92426758e+01 1.03430061e+02 3.68766189e-02]\n",
      " [7.40782013e+01 1.08794891e+02 2.30454296e-01]\n",
      " [6.51660385e+01 9.50761414e+01 2.25870535e-01]\n",
      " [5.89666672e+01 8.53156815e+01 2.05346167e-01]\n",
      " [6.83161774e+01 6.78478851e+01 1.62610523e-02]\n",
      " [6.37360497e+01 5.56123161e+01 8.39144140e-02]\n",
      " [5.94401436e+01 5.07513580e+01 1.13593161e-01]\n",
      " [4.38576012e+01 4.88173561e+01 3.31744999e-01]\n",
      " [7.61119080e+01 3.42391090e+01 3.32023382e-01]\n",
      " [8.13593750e+01 3.36437035e+01 2.83722699e-01]\n",
      " [7.51694870e+01 3.70328217e+01 2.31655598e-01]\n",
      " [7.93375931e+01 1.06330231e+02 1.63204059e-01]\n",
      " [5.62017441e+01 9.93024139e+01 3.31634879e-02]\n",
      " [7.61251068e+01 7.43594818e+01 3.05806268e-02]\n",
      " [8.91472321e+01 6.50840836e+01 0.00000000e+00]\n",
      " [9.16396866e+01 1.12847656e+02 1.01824880e-01]\n",
      " [7.04151306e+01 6.13657646e+01 3.73425744e-02]\n",
      " [8.06759872e+01 9.07684555e+01 9.24085751e-02]\n",
      " [8.95629044e+01 1.02552116e+02 2.16997415e-01]\n",
      " [9.47027893e+01 1.11155174e+02 9.32730064e-02]\n",
      " [5.55676460e+01 8.59058838e+01 4.96658757e-02]\n",
      " [7.65785141e+01 8.06266861e+01 0.00000000e+00]\n",
      " [5.89461479e+01 6.55487747e+01 0.00000000e+00]\n",
      " [5.15564842e+01 6.97566605e+01 2.84976140e-02]\n",
      " [5.43737984e+01 6.59919434e+01 1.01034515e-01]\n",
      " [4.88194733e+01 4.77879372e+01 2.78687119e-01]\n",
      " [4.68990364e+01 8.36243744e+01 0.00000000e+00]\n",
      " [7.54874725e+01 8.69746170e+01 0.00000000e+00]\n",
      " [8.33278656e+01 7.49013596e+01 0.00000000e+00]\n",
      " [7.75456390e+01 1.05010559e+02 1.83609966e-02]\n",
      " [4.56856537e+01 5.42106133e+01 3.58286127e-02]\n",
      " [6.47616577e+01 5.87445679e+01 4.96797971e-02]\n",
      " [5.87768059e+01 9.00252533e+01 0.00000000e+00]]\n",
      "6a1cb6d5-80894741-d6dd8184-01883160-eac23c2e.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "11\n",
      "##########################################\n",
      "[[64.         64.          0.        ]\n",
      " [29.30986977 63.66427231  0.17017204]\n",
      " [33.03822708 59.92880249  0.23769106]\n",
      " [28.74265671 73.81066132  0.29585364]\n",
      " [32.74645615 73.79587555  0.33752474]\n",
      " [24.61509705 75.26868439  0.2653273 ]\n",
      " [21.19254494 65.11460114  0.25533649]\n",
      " [27.94378471 70.86618805  0.29610816]\n",
      " [42.54973602 87.20767975  0.18961732]\n",
      " [53.90089035 85.95103455  0.16833709]\n",
      " [67.78660583 91.59837341  0.10767049]]\n",
      "2e91d8f4-8ed7f9cd-05e24f7f-782051c7-41b0859e.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "46\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.53659642e-01]\n",
      " [6.20019951e+01 7.08288345e+01 2.19042093e-01]\n",
      " [6.13628349e+01 4.63710518e+01 1.91741943e-01]\n",
      " [6.32489128e+01 5.27383690e+01 2.94648051e-01]\n",
      " [6.03461838e+01 4.41558571e+01 3.47106278e-01]\n",
      " [5.59608574e+01 8.23713303e+01 2.15698391e-01]\n",
      " [6.71785202e+01 8.60082550e+01 2.26401344e-01]\n",
      " [5.73340187e+01 5.33085365e+01 3.11528891e-01]\n",
      " [4.95919533e+01 4.29891052e+01 3.44975978e-01]\n",
      " [5.73031769e+01 4.68840942e+01 3.31172884e-01]\n",
      " [5.55246544e+01 4.77145615e+01 3.54951024e-01]\n",
      " [5.73294449e+01 4.84917870e+01 3.51630569e-01]\n",
      " [5.07410049e+01 4.84570885e+01 3.45115036e-01]\n",
      " [4.98138542e+01 4.40998344e+01 3.69920254e-01]\n",
      " [5.43871040e+01 3.76150093e+01 3.54091406e-01]\n",
      " [5.11925316e+01 4.26412849e+01 3.06901783e-01]\n",
      " [4.84659920e+01 4.17683983e+01 3.21710795e-01]\n",
      " [5.76916275e+01 3.49295464e+01 3.92637342e-01]\n",
      " [5.59513664e+01 5.08195877e+01 2.54989684e-01]\n",
      " [4.56293449e+01 3.67048836e+01 3.86293769e-01]\n",
      " [5.29449387e+01 3.51372490e+01 4.02908623e-01]\n",
      " [6.74128494e+01 3.49387970e+01 3.52777809e-01]\n",
      " [7.09544830e+01 5.97052307e+01 2.57524729e-01]\n",
      " [6.77348099e+01 6.35521584e+01 1.78735837e-01]\n",
      " [6.28762207e+01 7.61633377e+01 1.98832452e-01]\n",
      " [7.05320816e+01 8.26493301e+01 8.81586522e-02]\n",
      " [5.80816956e+01 9.03284378e+01 2.46220261e-01]\n",
      " [5.92563362e+01 9.30148087e+01 1.76123813e-01]\n",
      " [7.22689362e+01 5.29190445e+01 7.25027826e-03]\n",
      " [5.66288109e+01 7.80305252e+01 2.17516631e-01]\n",
      " [5.37309761e+01 8.04114304e+01 1.33813888e-01]\n",
      " [4.70631905e+01 6.02527542e+01 1.99634373e-01]\n",
      " [4.90393410e+01 7.65650864e+01 2.09468588e-01]\n",
      " [6.75266037e+01 8.65780029e+01 1.12206079e-01]\n",
      " [6.92896881e+01 7.06534119e+01 1.08736418e-01]\n",
      " [5.98050690e+01 9.02947922e+01 6.73635527e-02]\n",
      " [4.85979614e+01 6.89573898e+01 2.83544064e-01]\n",
      " [6.25244865e+01 7.26899719e+01 2.49111503e-01]\n",
      " [7.19856339e+01 7.26609344e+01 1.58209294e-01]\n",
      " [8.63054504e+01 4.33176956e+01 2.22384155e-01]\n",
      " [8.59892578e+01 4.23602867e+01 2.23994195e-01]\n",
      " [5.19560432e+01 7.32756195e+01 2.14855298e-01]\n",
      " [7.74459457e+01 9.46249847e+01 3.63374539e-02]\n",
      " [5.91844177e+01 1.07161758e+02 3.61942500e-02]\n",
      " [6.97706375e+01 1.03964386e+02 1.88963525e-02]\n",
      " [4.96707954e+01 9.17064590e+01 9.56681222e-02]]\n",
      "a083818a-3c2e6c0c-3562d09b-23e51f15-fd0b4109.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "28\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.75908372e-01]\n",
      " [5.15902557e+01 6.16408615e+01 2.88603663e-01]\n",
      " [5.90890350e+01 5.23333855e+01 2.47536987e-01]\n",
      " [5.77953682e+01 6.76211777e+01 2.66184121e-01]\n",
      " [5.01719322e+01 6.68459625e+01 3.09541851e-01]\n",
      " [4.87749329e+01 6.51838913e+01 3.35173100e-01]\n",
      " [5.33043785e+01 6.84548721e+01 3.54672432e-01]\n",
      " [5.93054466e+01 6.93812637e+01 3.51889640e-01]\n",
      " [5.40963898e+01 7.63588257e+01 3.05352777e-01]\n",
      " [4.96291084e+01 7.12047119e+01 3.56013834e-01]\n",
      " [5.98106270e+01 6.24641838e+01 2.94617623e-01]\n",
      " [5.86381035e+01 6.36930275e+01 3.36225003e-01]\n",
      " [6.11721268e+01 5.23328590e+01 3.49523067e-01]\n",
      " [5.74159012e+01 4.53602753e+01 2.75943637e-01]\n",
      " [5.94998207e+01 5.54425812e+01 3.03640038e-01]\n",
      " [6.03050346e+01 4.71273918e+01 3.18544894e-01]\n",
      " [5.44065285e+01 4.86948051e+01 3.45273435e-01]\n",
      " [5.85908775e+01 3.94169807e+01 3.59839499e-01]\n",
      " [5.62036552e+01 4.77092400e+01 3.04294556e-01]\n",
      " [6.17554855e+01 3.89889145e+01 3.98035169e-01]\n",
      " [5.05484161e+01 6.71377716e+01 2.91724801e-01]\n",
      " [6.29779930e+01 6.03793869e+01 1.72566310e-01]\n",
      " [6.30848808e+01 5.32907028e+01 2.07407027e-01]\n",
      " [5.52043419e+01 5.39041519e+01 2.78282523e-01]\n",
      " [4.34501953e+01 5.05166969e+01 3.02687973e-01]\n",
      " [6.54337006e+01 4.21489677e+01 3.07644457e-01]\n",
      " [5.25184784e+01 9.96550293e+01 1.18899174e-01]\n",
      " [6.39705887e+01 8.67805786e+01 9.58879292e-02]]\n",
      "d696dc20-e7e4cf9f-79bbc30b-4ab4a4bf-1429be71.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "35\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.76136756e-01]\n",
      " [6.36521988e+01 6.42595673e+01 2.77004242e-01]\n",
      " [6.92090988e+01 6.59394836e+01 2.37928674e-01]\n",
      " [6.68469086e+01 6.70861740e+01 2.99165249e-01]\n",
      " [7.00930710e+01 6.53448029e+01 3.34444284e-01]\n",
      " [7.51585159e+01 6.87562408e+01 2.18690112e-01]\n",
      " [7.93631134e+01 7.55245209e+01 1.58632785e-01]\n",
      " [6.34331169e+01 6.84385452e+01 3.61940384e-01]\n",
      " [5.03495178e+01 6.45603485e+01 3.31088573e-01]\n",
      " [5.13460464e+01 7.11622849e+01 3.63640636e-01]\n",
      " [5.14132576e+01 7.19261322e+01 3.37826610e-01]\n",
      " [6.09264908e+01 6.46471024e+01 3.65182757e-01]\n",
      " [8.70692673e+01 6.95085297e+01 1.57403380e-01]\n",
      " [6.81252213e+01 3.78559380e+01 3.91982108e-01]\n",
      " [8.83377457e+01 6.89476700e+01 2.69152403e-01]\n",
      " [6.70590897e+01 6.33157158e+01 2.69788414e-01]\n",
      " [5.77795906e+01 5.79166603e+01 2.77722716e-01]\n",
      " [7.00598679e+01 3.12949238e+01 4.27163482e-01]\n",
      " [7.37058487e+01 7.04508896e+01 4.73107547e-02]\n",
      " [9.55638962e+01 1.13558525e+02 2.30641082e-01]\n",
      " [6.38600006e+01 3.49608078e+01 2.87758082e-01]\n",
      " [8.67489777e+01 2.62256794e+01 3.39037657e-01]\n",
      " [7.34066925e+01 5.74399643e+01 3.00876707e-01]\n",
      " [7.15039749e+01 6.01914139e+01 4.55404855e-02]\n",
      " [8.42921219e+01 7.11865692e+01 1.18313044e-01]\n",
      " [7.17753372e+01 6.79639969e+01 2.02062562e-01]\n",
      " [6.42663422e+01 5.22538147e+01 2.97786802e-01]\n",
      " [5.48098030e+01 7.32283554e+01 2.86056221e-01]\n",
      " [5.62078629e+01 4.03638802e+01 2.96584010e-01]\n",
      " [6.49776230e+01 5.38746567e+01 2.68599957e-01]\n",
      " [5.28412971e+01 7.44822998e+01 3.10185939e-01]\n",
      " [7.01448135e+01 4.78190155e+01 3.57396096e-01]\n",
      " [7.43991089e+01 7.71746979e+01 3.95297021e-01]\n",
      " [7.39107895e+01 4.82029457e+01 3.35248798e-01]\n",
      " [6.54177094e+01 4.92619057e+01 3.19593221e-01]]\n",
      "06a48361-9970c39e-e856216d-8ec297ff-07e62df1.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "18\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.33715117e-01]\n",
      " [5.06432495e+01 6.28978310e+01 3.87076139e-01]\n",
      " [5.84679794e+01 7.18393555e+01 2.84936339e-01]\n",
      " [4.50742264e+01 6.99899063e+01 3.02160859e-01]\n",
      " [4.05864067e+01 6.81996231e+01 3.54297966e-01]\n",
      " [4.58513374e+01 7.13924713e+01 3.32492799e-01]\n",
      " [4.58789787e+01 7.95431976e+01 3.53472799e-01]\n",
      " [4.03397522e+01 7.73541718e+01 3.94924760e-01]\n",
      " [3.67939873e+01 8.48010559e+01 4.12400931e-01]\n",
      " [3.99444618e+01 1.01957008e+02 2.63744712e-01]\n",
      " [5.98647270e+01 4.40716438e+01 3.15908819e-01]\n",
      " [5.01235313e+01 5.15801659e+01 3.86385888e-01]\n",
      " [3.75340652e+01 3.81921310e+01 3.40029061e-01]\n",
      " [5.88105774e+01 3.25963745e+01 2.70977765e-01]\n",
      " [5.90272446e+01 3.46112099e+01 3.21823508e-01]\n",
      " [5.23409958e+01 4.89454689e+01 2.58732527e-01]\n",
      " [7.63787842e+01 5.30503120e+01 1.58793956e-01]\n",
      " [1.00583626e+02 8.51722336e+01 2.46649445e-03]]\n",
      "a7719240-9ae3f703-c18562b2-e3bb20c6-1ab91a3c.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "26\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 0.00000000e+00]\n",
      " [5.94743576e+01 7.15299911e+01 2.21926779e-01]\n",
      " [6.65138092e+01 7.25048447e+01 2.51439720e-01]\n",
      " [6.80815887e+01 7.22768631e+01 3.07671189e-01]\n",
      " [1.03713531e+02 3.94917717e+01 2.16864482e-01]\n",
      " [9.07673035e+01 4.23467674e+01 2.89785624e-01]\n",
      " [9.39929276e+01 4.41831436e+01 2.25943550e-01]\n",
      " [8.97765732e+01 6.58836823e+01 2.34629154e-01]\n",
      " [8.01208801e+01 4.14438362e+01 2.50284106e-01]\n",
      " [7.73170319e+01 4.57702065e+01 2.70459831e-01]\n",
      " [8.54514465e+01 8.18287506e+01 2.57015973e-01]\n",
      " [8.59491959e+01 8.08701172e+01 2.82837242e-01]\n",
      " [8.52789764e+01 8.36390381e+01 1.31548047e-01]\n",
      " [7.15478516e+01 5.09614525e+01 2.83209682e-01]\n",
      " [7.97691193e+01 8.13042068e+01 2.26012051e-01]\n",
      " [6.93039017e+01 8.29393845e+01 2.60672212e-01]\n",
      " [6.53161316e+01 7.96062164e+01 3.01114947e-01]\n",
      " [7.87315216e+01 3.84634781e+01 2.36818522e-01]\n",
      " [7.94402390e+01 4.26553917e+01 2.46045128e-01]\n",
      " [9.34570618e+01 5.75254059e+01 1.87280983e-01]\n",
      " [9.44581528e+01 4.58748589e+01 1.46223545e-01]\n",
      " [9.10366058e+01 7.55755081e+01 6.09049723e-02]\n",
      " [6.35087128e+01 9.13009872e+01 1.51301622e-01]\n",
      " [5.33309059e+01 8.33421097e+01 2.18387187e-01]\n",
      " [5.82129517e+01 8.81615372e+01 1.73340872e-01]\n",
      " [5.90092659e+01 9.48559875e+01 1.80698633e-01]]\n",
      "643ac480-73cfe93c-27cea4d8-84722781-97d03ed4.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "47\n",
      "##########################################\n",
      "[[64.         64.          0.18988572]\n",
      " [43.66212845 66.99307251  0.32916757]\n",
      " [59.23413849 69.21697998  0.20310123]\n",
      " [52.63553619 74.421875    0.35687605]\n",
      " [47.84654999 82.58393097  0.36074853]\n",
      " [58.27090836 75.25022125  0.14103557]\n",
      " [84.24264526 79.67563629  0.20648558]\n",
      " [31.48087502 74.82497406  0.42614305]\n",
      " [28.541399   77.50234985  0.43008769]\n",
      " [54.94426346 66.83132172  0.40422308]\n",
      " [61.76872635 81.77813721  0.4012278 ]\n",
      " [56.07963943 77.39732361  0.38419151]\n",
      " [59.35589981 80.93825531  0.36825544]\n",
      " [60.01660156 74.70355988  0.26812264]\n",
      " [61.88829422 69.35069275  0.24457566]\n",
      " [67.61664581 89.75823212  0.33404619]\n",
      " [66.40818024 96.80908966  0.28792694]\n",
      " [56.85190582 78.96115875  0.32788128]\n",
      " [40.64572144 75.05329895  0.37285998]\n",
      " [54.63383484 65.54602814  0.31899539]\n",
      " [53.49380493 89.71496582  0.36549148]\n",
      " [56.4958725  84.83033752  0.35763273]\n",
      " [74.58815002 92.96836853  0.41910073]\n",
      " [81.37768555 25.98459053  0.44657886]\n",
      " [71.54187012 29.51462173  0.36506793]\n",
      " [55.97932434 80.20622253  0.36676145]\n",
      " [62.09107208 41.13774109  0.3862597 ]\n",
      " [50.52585602 55.49061584  0.30950984]\n",
      " [47.79671097 69.94346619  0.26184773]\n",
      " [65.46395874 98.72388458  0.2073594 ]\n",
      " [50.36506271 86.41257477  0.2006761 ]\n",
      " [48.2291069  41.59159851  0.36692864]\n",
      " [62.48522568 38.51659393  0.36874461]\n",
      " [39.94172287 77.0635376   0.29692665]\n",
      " [49.90753555 89.03101349  0.23617421]\n",
      " [46.25733185 46.20912933  0.34224132]\n",
      " [42.77974319 48.43160629  0.37730992]\n",
      " [43.12889481 40.69977951  0.35944274]\n",
      " [47.7863884  48.64351273  0.38216415]\n",
      " [57.7349205  59.80509186  0.15796475]\n",
      " [46.09448624 41.61044693  0.3444809 ]\n",
      " [42.55280685 41.03845596  0.36560443]\n",
      " [35.00674057 63.0269165   0.36626837]\n",
      " [45.64471436 72.55729675  0.23590922]\n",
      " [52.37360764 72.23984528  0.13802205]\n",
      " [65.68611145 41.48901367  0.35301432]\n",
      " [46.39771652 55.68195343  0.23678999]]\n",
      "d4710e90-03ad2ca8-87a21d8f-d59bf306-15a9db0f.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "46\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.91650951e-01]\n",
      " [5.04123650e+01 6.86352463e+01 2.66971916e-01]\n",
      " [6.03417740e+01 6.44296112e+01 2.39914432e-01]\n",
      " [6.12362213e+01 5.31440048e+01 2.78804123e-01]\n",
      " [5.24757843e+01 5.84813652e+01 3.41463983e-01]\n",
      " [5.47821770e+01 6.08324089e+01 3.33228230e-01]\n",
      " [5.15479317e+01 7.51382980e+01 3.65063727e-01]\n",
      " [5.13133736e+01 5.67997208e+01 3.72459829e-01]\n",
      " [4.59638405e+01 6.51024475e+01 4.29976076e-01]\n",
      " [5.06232529e+01 6.49834518e+01 3.57503027e-01]\n",
      " [5.48382492e+01 7.22152786e+01 3.29672366e-01]\n",
      " [4.86181526e+01 7.24126968e+01 3.21831018e-01]\n",
      " [4.29113846e+01 6.82408752e+01 3.34813714e-01]\n",
      " [4.91042252e+01 8.63328781e+01 2.67755330e-01]\n",
      " [5.79360237e+01 8.83201752e+01 1.92444548e-01]\n",
      " [5.53106079e+01 8.51403656e+01 3.57493311e-01]\n",
      " [5.51638985e+01 8.45277023e+01 3.04781497e-01]\n",
      " [6.85743637e+01 6.12638130e+01 3.26618135e-01]\n",
      " [5.39684486e+01 3.52787323e+01 3.23684484e-01]\n",
      " [5.42103920e+01 8.80518494e+01 2.31411725e-01]\n",
      " [4.74859848e+01 6.75649414e+01 3.31348002e-01]\n",
      " [4.21256180e+01 7.65888977e+01 4.37521815e-01]\n",
      " [4.67891312e+01 6.16273270e+01 3.10878932e-01]\n",
      " [5.78433304e+01 7.58989410e+01 3.06150347e-01]\n",
      " [6.76260300e+01 9.86137619e+01 2.75926679e-01]\n",
      " [5.81161385e+01 9.20845032e+01 2.64378488e-01]\n",
      " [4.00868988e+01 9.04848404e+01 3.40275705e-01]\n",
      " [4.12955246e+01 6.52495956e+01 3.10747832e-01]\n",
      " [6.72902069e+01 9.47172699e+01 2.55584031e-01]\n",
      " [8.81896133e+01 6.72543030e+01 4.17423517e-01]\n",
      " [8.11599503e+01 4.02751541e+01 4.25612450e-01]\n",
      " [8.28103409e+01 3.96959839e+01 2.82866746e-01]\n",
      " [7.45616913e+01 1.00357491e+02 1.20590933e-01]\n",
      " [7.52326736e+01 8.38379822e+01 9.16738361e-02]\n",
      " [6.24728546e+01 9.03828659e+01 1.66566104e-01]\n",
      " [4.28645782e+01 7.84009781e+01 2.53058314e-01]\n",
      " [4.52993736e+01 5.86529121e+01 3.12529385e-01]\n",
      " [6.89860153e+01 2.97673912e+01 3.52458268e-01]\n",
      " [5.53568954e+01 5.46489563e+01 3.30562294e-01]\n",
      " [5.76027527e+01 3.70204849e+01 3.48302662e-01]\n",
      " [6.28852310e+01 5.29980774e+01 3.63617420e-01]\n",
      " [5.72117538e+01 1.00331558e+02 2.22647160e-01]\n",
      " [6.35990143e+01 8.25630112e+01 1.64439142e-01]\n",
      " [7.89502411e+01 1.00601357e+02 2.06956998e-01]\n",
      " [8.37401657e+01 1.12863182e+02 3.17993075e-01]\n",
      " [6.10198746e+01 6.71789551e+01 4.40220833e-01]]\n",
      "9c926068-8ff0f63a-fa66a31d-cf2c4b81-8da17e07.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "27\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 0.00000000e+00]\n",
      " [4.64989471e+01 5.71406708e+01 2.77099997e-01]\n",
      " [5.88486862e+01 7.10420227e+01 1.06196150e-01]\n",
      " [6.96522827e+01 7.58239136e+01 1.50827631e-01]\n",
      " [5.33242455e+01 6.23402748e+01 2.66846359e-01]\n",
      " [4.02990761e+01 6.43416061e+01 3.00269663e-01]\n",
      " [4.64399109e+01 6.14415627e+01 2.76153296e-01]\n",
      " [5.90179062e+01 5.02713509e+01 2.77747869e-01]\n",
      " [5.92670555e+01 4.52053947e+01 2.81651139e-01]\n",
      " [5.25055809e+01 5.26845245e+01 1.99775234e-01]\n",
      " [5.48475304e+01 6.40616989e+01 2.56892651e-01]\n",
      " [5.27761803e+01 4.96283112e+01 3.12055230e-01]\n",
      " [5.88995819e+01 4.58370361e+01 2.62661934e-01]\n",
      " [6.28476486e+01 4.57195320e+01 2.59249896e-01]\n",
      " [6.04826126e+01 9.01790695e+01 2.62949429e-02]\n",
      " [6.01604233e+01 5.10319824e+01 1.74528390e-01]\n",
      " [7.12664948e+01 4.68084373e+01 2.76587039e-01]\n",
      " [6.73193054e+01 4.53431206e+01 2.26844653e-01]\n",
      " [6.33278847e+01 9.48860855e+01 0.00000000e+00]\n",
      " [6.29689178e+01 8.17325592e+01 0.00000000e+00]\n",
      " [5.77864609e+01 5.96342278e+01 1.91942662e-01]\n",
      " [6.24349136e+01 8.72068253e+01 1.65129781e-01]\n",
      " [6.35003204e+01 8.89648819e+01 7.15154335e-02]\n",
      " [6.53053207e+01 1.05866722e+02 1.39840990e-01]\n",
      " [6.70011063e+01 3.85398521e+01 3.16027761e-01]\n",
      " [6.97578735e+01 7.08630600e+01 4.76996787e-02]\n",
      " [7.01933594e+01 7.35126572e+01 5.56943230e-02]]\n",
      "570e2641-afb84e9f-3dc49429-7cdd3e3f-4f5d1514.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "41\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 1.43770754e-01]\n",
      " [7.63601151e+01 7.03613815e+01 1.74415916e-01]\n",
      " [8.57339401e+01 6.59911652e+01 1.35486513e-01]\n",
      " [8.55456085e+01 6.82192688e+01 1.31652132e-01]\n",
      " [7.52213821e+01 7.26618195e+01 1.80050954e-01]\n",
      " [9.94493103e+01 9.49647903e+01 0.00000000e+00]\n",
      " [1.13116173e+02 1.03823311e+02 0.00000000e+00]\n",
      " [8.69435425e+01 7.30975647e+01 2.03894377e-01]\n",
      " [8.57076263e+01 8.76245804e+01 2.90210247e-01]\n",
      " [9.25621948e+01 8.89892654e+01 1.71752691e-01]\n",
      " [7.61966019e+01 6.43307571e+01 1.96743354e-01]\n",
      " [7.00337219e+01 5.88031158e+01 3.11801255e-01]\n",
      " [7.85613480e+01 5.39394112e+01 2.21393183e-01]\n",
      " [8.28193588e+01 5.44634247e+01 2.48017386e-01]\n",
      " [8.43258667e+01 5.22025414e+01 3.07659596e-01]\n",
      " [7.60308380e+01 7.58317337e+01 6.32572547e-02]\n",
      " [8.75404282e+01 4.65268898e+01 2.82421410e-01]\n",
      " [7.80082016e+01 6.64738007e+01 2.50023574e-01]\n",
      " [8.64963379e+01 7.84237900e+01 7.42714778e-02]\n",
      " [7.53031158e+01 8.23627472e+01 2.96838641e-01]\n",
      " [8.33295441e+01 7.85997086e+01 1.96720362e-01]\n",
      " [8.74041748e+01 6.41480331e+01 2.05443993e-01]\n",
      " [9.05438080e+01 6.83149414e+01 1.39162272e-01]\n",
      " [1.01128807e+02 3.80470772e+01 1.25868767e-01]\n",
      " [8.27255020e+01 3.83432846e+01 1.97993815e-01]\n",
      " [9.31733780e+01 3.39582176e+01 2.01208636e-01]\n",
      " [8.95871506e+01 3.43246460e+01 3.27505112e-01]\n",
      " [8.40064087e+01 4.41714478e+01 2.20659405e-01]\n",
      " [9.75693359e+01 5.28090439e+01 0.00000000e+00]\n",
      " [1.12629166e+02 3.49327164e+01 5.97010329e-02]\n",
      " [1.20133339e+02 9.11029053e+01 2.80118920e-03]\n",
      " [9.34673920e+01 4.21639175e+01 2.55308270e-01]\n",
      " [9.72507019e+01 4.31020622e+01 1.38214782e-01]\n",
      " [8.85095901e+01 6.27593117e+01 6.38036756e-03]\n",
      " [9.17668304e+01 6.35739746e+01 1.01025358e-01]\n",
      " [6.47942047e+01 4.19693222e+01 3.37143421e-01]\n",
      " [6.19012032e+01 4.75421257e+01 2.37075254e-01]\n",
      " [8.01507568e+01 6.03970604e+01 2.28409782e-01]\n",
      " [8.21324234e+01 6.04208946e+01 2.58652687e-01]\n",
      " [9.39123306e+01 8.06990356e+01 3.77800241e-02]\n",
      " [6.41633453e+01 5.96421967e+01 1.76225603e-01]]\n",
      "19fc9e56-b2cd2486-cbfd7926-ae2d4343-2228e959.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "50\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 0.00000000e+00]\n",
      " [7.57457886e+01 6.15482063e+01 0.00000000e+00]\n",
      " [6.92826385e+01 6.98849640e+01 7.65747204e-02]\n",
      " [6.78818512e+01 5.59525146e+01 2.52302527e-01]\n",
      " [7.54476471e+01 7.32141571e+01 2.69007444e-01]\n",
      " [9.18129578e+01 1.02496498e+02 5.70152104e-02]\n",
      " [9.51434784e+01 9.13886642e+01 1.76656432e-02]\n",
      " [1.00152954e+02 7.65085144e+01 0.00000000e+00]\n",
      " [9.86627502e+01 1.00419899e+02 1.92701444e-01]\n",
      " [7.53927612e+01 9.02551270e+01 2.20383145e-02]\n",
      " [7.51850967e+01 9.24538193e+01 9.22395214e-02]\n",
      " [7.46892014e+01 8.62063446e+01 8.85052308e-02]\n",
      " [6.61373672e+01 8.50156097e+01 1.12376362e-01]\n",
      " [6.50961838e+01 6.66134796e+01 2.35704198e-01]\n",
      " [7.36313782e+01 6.19422417e+01 1.94770440e-01]\n",
      " [6.66465607e+01 7.21231842e+01 1.25608444e-01]\n",
      " [9.01487885e+01 5.63405533e+01 1.25182137e-01]\n",
      " [5.81697464e+01 8.79381256e+01 2.98618019e-01]\n",
      " [5.23419304e+01 8.92010193e+01 2.62031704e-01]\n",
      " [5.63896790e+01 8.56117477e+01 2.50419557e-01]\n",
      " [6.99318924e+01 8.19134903e+01 1.28860757e-01]\n",
      " [8.21624985e+01 7.65067291e+01 1.45871341e-01]\n",
      " [6.04148750e+01 7.18445129e+01 1.59760535e-01]\n",
      " [7.80862579e+01 7.39646454e+01 1.40234223e-02]\n",
      " [7.72411957e+01 8.30182495e+01 4.62765209e-02]\n",
      " [7.25813980e+01 8.14086761e+01 5.18348068e-02]\n",
      " [8.70896454e+01 8.38960266e+01 1.50290176e-01]\n",
      " [9.45736313e+01 9.02282181e+01 8.10439363e-02]\n",
      " [7.28796310e+01 1.06588242e+02 1.98633388e-01]\n",
      " [8.60615845e+01 7.62893372e+01 5.57825863e-02]\n",
      " [7.27861252e+01 7.55426712e+01 0.00000000e+00]\n",
      " [6.56404648e+01 7.48636093e+01 1.13846473e-01]\n",
      " [7.27654800e+01 8.81923676e+01 1.10769995e-01]\n",
      " [6.73454895e+01 6.37303619e+01 1.09437548e-01]\n",
      " [7.69998703e+01 6.65930405e+01 3.79828364e-02]\n",
      " [7.81853485e+01 8.52603607e+01 0.00000000e+00]\n",
      " [7.80003586e+01 9.22886200e+01 7.62949139e-02]\n",
      " [5.71521492e+01 7.25020752e+01 1.98773637e-01]\n",
      " [7.80928192e+01 9.66450882e+01 1.55354440e-01]\n",
      " [8.04170380e+01 3.95105247e+01 3.05919826e-01]\n",
      " [8.26859283e+01 3.16692333e+01 2.33131319e-01]\n",
      " [8.04490662e+01 4.73765373e+01 1.18983030e-01]\n",
      " [8.26260910e+01 3.38894386e+01 3.15683156e-01]\n",
      " [6.86093903e+01 7.50964737e+01 2.19724089e-01]\n",
      " [6.85685349e+01 8.51907425e+01 2.15862721e-01]\n",
      " [8.65748520e+01 3.06901550e+01 2.61704981e-01]\n",
      " [8.79210968e+01 9.16582642e+01 9.55103710e-02]\n",
      " [7.96390076e+01 6.58803101e+01 1.32317618e-01]\n",
      " [6.26274986e+01 8.31074219e+01 1.35755345e-01]\n",
      " [5.92135391e+01 6.09315796e+01 2.64256120e-01]]\n",
      "17657a41-380cafa1-ac9a7c3c-7863d080-5eece243.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "37\n",
      "##########################################\n",
      "[[6.40000000e+01 6.40000000e+01 2.38173921e-03]\n",
      " [6.96088409e+01 6.93819046e+01 1.66572243e-01]\n",
      " [7.12399597e+01 6.95770264e+01 1.83029830e-01]\n",
      " [7.03093414e+01 5.43790283e+01 2.82270104e-01]\n",
      " [7.94227448e+01 4.82753372e+01 2.75542080e-01]\n",
      " [8.37798004e+01 6.74067154e+01 1.22320086e-01]\n",
      " [7.63281631e+01 6.75969772e+01 1.20264418e-01]\n",
      " [7.84386292e+01 5.66419106e+01 2.32220039e-01]\n",
      " [8.08728638e+01 6.50641632e+01 1.87400669e-01]\n",
      " [8.53519363e+01 9.23279648e+01 7.89402723e-02]\n",
      " [7.99003601e+01 8.86906204e+01 0.00000000e+00]\n",
      " [1.00978592e+02 1.10064522e+02 9.01127160e-02]\n",
      " [7.48906937e+01 5.39364738e+01 1.65895507e-01]\n",
      " [7.43554382e+01 4.44549408e+01 4.93941084e-02]\n",
      " [7.89703369e+01 9.40563507e+01 0.00000000e+00]\n",
      " [7.13501282e+01 9.36141663e+01 1.51798055e-01]\n",
      " [5.50764313e+01 9.41684418e+01 2.93554395e-01]\n",
      " [6.14840050e+01 8.86118317e+01 1.40557677e-01]\n",
      " [5.52667656e+01 7.90850830e+01 1.55194432e-01]\n",
      " [6.15777931e+01 9.82768860e+01 2.19708264e-01]\n",
      " [5.79591408e+01 1.01798096e+02 1.63797945e-01]\n",
      " [7.17212372e+01 8.53722000e+01 1.49179772e-01]\n",
      " [6.12924728e+01 8.37246246e+01 1.33760259e-01]\n",
      " [7.06012421e+01 1.08680107e+02 2.59867162e-01]\n",
      " [6.48128128e+01 1.05205780e+02 1.38413921e-01]\n",
      " [5.67892761e+01 8.68706436e+01 1.22119986e-01]\n",
      " [6.80253677e+01 6.92940979e+01 1.69784576e-01]\n",
      " [6.68263626e+01 7.67801056e+01 9.39388722e-02]\n",
      " [6.45761795e+01 4.60142326e+01 0.00000000e+00]\n",
      " [5.52168617e+01 3.15791187e+01 3.61994892e-01]\n",
      " [7.45102081e+01 1.68875122e+01 3.21277291e-01]\n",
      " [7.78250656e+01 3.37670403e+01 3.22993547e-01]\n",
      " [6.61287613e+01 7.76185532e+01 9.40148234e-02]\n",
      " [6.61218338e+01 8.64563751e+01 1.69619143e-01]\n",
      " [5.12099113e+01 5.23728142e+01 2.65333474e-01]\n",
      " [5.45407181e+01 4.55269051e+01 2.65773475e-01]\n",
      " [5.77049370e+01 6.50275345e+01 2.13585645e-01]]\n",
      "0f502a52-7d8fb5f0-91fd0fa6-39f88d77-b6f57c7f.jpg\n",
      "printing input embeddingskfksf\n",
      "#####################################################################\n",
      "1\n",
      "21\n",
      "##########################################\n",
      "[[64.         64.          0.        ]\n",
      " [55.92073822 67.78847504  0.14897884]\n",
      " [39.66987991 60.23448181  0.33676228]\n",
      " [44.91253281 60.61872101  0.34075356]\n",
      " [53.99171066 58.70388794  0.23889464]\n",
      " [49.9580307  58.66362     0.24311778]\n",
      " [47.19265366 65.15982056  0.22966076]\n",
      " [42.85542297 73.84177399  0.37460878]\n",
      " [48.89704132 66.61792755  0.39772472]\n",
      " [51.51303482 67.3915329   0.29262546]\n",
      " [36.85375595 73.19055939  0.35575917]\n",
      " [40.61731339 67.74848175  0.31169969]\n",
      " [47.16981506 63.91319656  0.22897929]\n",
      " [48.24100113 77.70894623  0.15725346]\n",
      " [41.62709045 76.66584015  0.24124491]\n",
      " [34.35017395 76.30667877  0.39012039]\n",
      " [45.10124588 83.56958008  0.28484425]\n",
      " [38.32775116 84.17188263  0.09143272]\n",
      " [46.06752777 42.4046669   0.24294171]\n",
      " [28.28154945 49.58657074  0.31095904]\n",
      " [35.67391586 56.42175293  0.31720304]]\n",
      "8e457921-bc1af8aa-a65073c1-aaac8247-c5ceb780.jpg\n",
      "printing input embeddingskfksf\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from os.path import join\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def postprocessScanpaths(trajs):\n",
    "    # convert actions to scanpaths\n",
    "    scanpaths = []\n",
    "    for traj in trajs:\n",
    "        task_name, img_name, condition, subject, fixs = traj\n",
    "        scanpaths.append({\n",
    "            'X': fixs[:, 1],\n",
    "            'Y': fixs[:, 0],\n",
    "            'T': fixs[:, 2],\n",
    "            'subject':subject,\n",
    "            'name': img_name,\n",
    "            'task': task_name,\n",
    "            'condition': condition\n",
    "        })\n",
    "    return scanpaths\n",
    "    \n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def run_model(model, src, task, device = \"cuda:0\", im_h=8, im_w=8, patch_size = 16, num_samples = 1):\n",
    "    src = src.to(device)\n",
    "    task =task\n",
    "    firstfix = torch.tensor([(im_h//2)*patch_size, (im_w//2)*patch_size]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        token_prob, ys, xs, ts = model(src = src, tgt = firstfix, task = task)\n",
    "    token_prob = token_prob.detach().cpu().numpy()\n",
    "    ys = ys.cpu().detach().numpy()\n",
    "    xs = xs.cpu().detach().numpy()\n",
    "    ts = ts.cpu().detach().numpy()\n",
    "    scanpaths = []\n",
    "    for i in range(num_samples):\n",
    "        ys_i = [(im_h//2) * patch_size] + list(ys[:, i, 0])[1:]\n",
    "        xs_i = [(im_w//2) * patch_size] + list(xs[:, i, 0])[1:]\n",
    "        ts_i = list(ts[:, i, 0])\n",
    "        token_type = [0] + list(np.argmax(token_prob[:, i, :], axis=-1))[1:]\n",
    "        scanpath = []\n",
    "        for tok, y, x, t in zip(token_type, ys_i, xs_i, ts_i):\n",
    "            if tok == 0:\n",
    "                scanpath.append([y,x, t])\n",
    "            else:\n",
    "                break\n",
    "        scanpaths.append(np.array(scanpath))\n",
    "        \n",
    "    return scanpaths\n",
    "    \n",
    "    \n",
    "def test(args):\n",
    "    trained_model = args.model_root\n",
    "    #device = torch.device('cpu')\n",
    "    device = torch.device('cuda:{}'.format(args.cuda))\n",
    "    transformer = Transformer(num_encoder_layers=args.num_encoder, nhead = args.nhead, d_model = args.hidden_dim, num_decoder_layers=args.num_decoder, dim_feedforward = args.hidden_dim, img_hidden_dim = args.img_hidden_dim, lm_dmodel = args.lm_hidden_dim, device = device).to(device)\n",
    "    model = medgaze(transformer = transformer, spatial_dim = (args.im_h, args.im_w), max_len = args.max_len, device = device).to(device)\n",
    "    model.load_state_dict(torch.load(trained_model, map_location=device)['model'])\n",
    "    model.eval()\n",
    "    dataset_root = args.dataset_dir\n",
    "    img_ftrs_dir = args.img_ftrs_dir\n",
    "    max_len = args.max_len\n",
    "    fixation_path = join(dataset_root, args.valid_file)\n",
    "    \n",
    "    with open(fixation_path) as json_file:\n",
    "        human_scanpaths = json.load(json_file)\n",
    "    test_target_trajs =  human_scanpaths\n",
    "    #print(test_target_trajs)\n",
    "    #test_task_img_pairs = np.unique([traj['task'] + '_' + traj['name'] + '_' + traj['condition'] for traj in test_target_trajs])\n",
    "    embedding_dict = x\n",
    "     \n",
    "    pred_list = []\n",
    "    print('Generating {} scanpaths per test case...'.format(1))\n",
    "    for target_traj in test_target_trajs:\n",
    "    #task_name, name, condition = target_traj.split('_')\n",
    "            #vb=Blip2ImageTrainProcessor.from_config()\n",
    "\n",
    "            #from PIL import Image\n",
    "            print(target_traj['name'])\n",
    "            \n",
    "            image_ftrs = torch.load(join(img_ftrs_dir, target_traj['name'].replace('jpg','pth'))).unsqueeze(0)\n",
    "            task_emb = [x.item()[target_traj['name'][:-4]]]\n",
    "\n",
    "            scanpaths = run_model(model=model, src=image_ftrs, task=task_emb, device=device, num_samples=1)\n",
    "            print(\"#####################################################################\")\n",
    "            print(len(scanpaths))\n",
    "            print(len(scanpaths[0]))\n",
    "            print(\"##########################################\")\n",
    "            for idx, scanpath in enumerate(scanpaths):\n",
    "                       print(scanpath)\n",
    "                       pred_list.append((target_traj['task'], target_traj['name'], target_traj['condition'], idx+1, scanpath))\n",
    "\n",
    "    predictions = postprocessScanpaths(pred_list)\n",
    "    return predictions \n",
    "    #print(predictions)\n",
    "    #torch.save(predictions,'norm_prednew.pth')\n",
    "\n",
    "    \n",
    "\n",
    "#seed_everything(args.seed)\n",
    "import pandas as pd\n",
    "args=pd.Series(args)\n",
    "predict=test(args)\n",
    "    #print('Sequence Score : {:.3f}, Sequence Score with Duration : {:.3f}'.format(seq_score, seq_score_t))\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5302c",
   "metadata": {},
   "source": [
    "# IOU and  CC  Calculation for different combination of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "size=128\n",
    "fixation_path = args.valid_file\n",
    "    \n",
    "with open(fixation_path) as json_file:\n",
    "        human_scanpaths = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe32d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dictv = {item['name']:item for item in human_scanpaths}\n",
    "new_dictp = {item['name']:item for item in predict}\n",
    "\n",
    "iou_mean1=[]\n",
    "nss_mean1=[]\n",
    "cc_mean1=[]\n",
    "for ipt in range(50, 51):\n",
    "            iou_score=[]\n",
    "            nss_score=[]\n",
    "            cc_score=[]\n",
    "            for rt in new_dictv.items() :\n",
    "                        did= rt[0]\n",
    "                        inp=[]\n",
    "                        #i=0\n",
    "                        for i in range(len(list(new_dictp[did]['X']))):\n",
    "                             inp.append([list(new_dictp[did]['X'])[i],list(new_dictp[did]['Y'])[i],list(new_dictp[did]['T'])[i]])\n",
    "                        #stop=len(inp)\n",
    "                        heatp=heatmap(inp,ipt)\n",
    "                        inp=[]\n",
    "                        #new_dict=new_dictp[0]\n",
    "                        i=0\n",
    "                        for i in range(len(list(new_dictv[did]['X']))):\n",
    "                             inp.append([list(new_dictv[did]['X'])[i],list(new_dictv[did]['Y'])[i],list(new_dictv[did]['T'])[i]])\n",
    "                        #if len(inp)>50:\n",
    "                        #    continue\n",
    "                        heatv=heatmap(inp,ipt)  \n",
    "                        we=heatv.copy()\n",
    "                        i=0\n",
    "                        #heatp[heatp > 0] = 1\n",
    "                        #heatv[heatv > 0] = 1\n",
    "                        for i in range(size):\n",
    "                            for j in range(size):\n",
    "\n",
    "                                    if we[i,j]>0:\n",
    "                                            #print('g')\n",
    "                                            we[i,j]=1\n",
    "                                    else:\n",
    "                                            we[i,j]=0\n",
    "\n",
    "\n",
    "                        gt= we.copy()   \n",
    "                        we=heatp.copy()\n",
    "                        for i in range(size):\n",
    "                            for j in range(size):\n",
    "\n",
    "                                    if we[i,j]>0:\n",
    "                                            #print('g')\n",
    "                                            we[i,j]=1\n",
    "                                    else:\n",
    "                                            we[i,j]=0\n",
    "                        prediction= we.copy()\n",
    "                        #gt=heatv\n",
    "                        #prediction=heatp\n",
    "                        #print(type(nss(gt, prediction)))\n",
    "                        if math.isnan(nss(gt, prediction)):\n",
    "                                    #print('ff')\n",
    "                                    nss_score.append(0)\n",
    "                        else:\n",
    "                                    nss_score.append(nss(gt, prediction))\n",
    "                        if math.isnan(cc(gt, prediction)):\n",
    "                                    #print(cc(gt, prediction))\n",
    "                                    cc_score.append(0)\n",
    "                        else:\n",
    "                                    cc_score.append(cc(gt, prediction))\n",
    "                        intersection = np.logical_and(gt, prediction)\n",
    "                        union = np.logical_or(gt, prediction)\n",
    "                        iou_scor = np.sum(intersection) / np.sum(union)\n",
    "                        iou_score.append(iou_scor)\n",
    "            iou_mean1.append(np.mean(np.array(iou_score)))\n",
    "            nss_mean1.append(np.mean(np.array(nss_score)))\n",
    "            cc_mean1.append(np.mean(np.array(cc_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28286b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49303518640116506]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_mean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7b9cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f04e54e7910>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as plt\n",
    "plt.plot(iou_mean1)\n",
    "plt.xlabel('Spread')\n",
    "plt.ylabel('Score')\n",
    "plt.legend('nIoU')\n",
    "plt.plot(cc_mean1)\n",
    "plt.legend('nCC')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
